
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":["admin"],"categories":null,"content":"I am an Assistant Professor in applied mathematics at ENSTA Paris.\nMy research interests include fluid mechanics, P.D.E analysis, numerical schemes, reduced order methods, and inverse problems.\nI am currently developping a website on reduced basis methods: link.\n","date":1717977600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1717977600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am an Assistant Professor in applied mathematics at ENSTA Paris.\nMy research interests include fluid mechanics, P.D.E analysis, numerical schemes, reduced order methods, and inverse problems.\nI am currently developping a website on reduced basis methods: link.","tags":null,"title":"Elise Grosjean","type":"authors"},{"authors":["Elise Grosjean"],"categories":null,"content":"","date":1741046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1741046400,"objectID":"ef984bb52975d33372612fe147395aa5","permalink":"https://grosjean1.github.io/talk/seminaire-irma-strasbourg/","publishdate":"2025-03-04T00:00:00Z","relpermalink":"/talk/seminaire-irma-strasbourg/","section":"event","summary":"Analyse de sensibilité et Méthodes de base réduite non intrusives","tags":[],"title":"Séminaire IRMA - Strasbourg","type":"event"},{"authors":[],"categories":[],"content":"#Install package import sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib # import packages import numpy as np import scipy from scipy import optimize from scipy.optimize import minimize Deterministic model Time scale: the decisions are taken every hour during the day.\nOptimization variables (at each time step) :\nthe amount of energy to be stored or withdrawn from the battery the amount of energy bought or sold to the network. Contraints:\nnonnegativity of variables evolution of the battery storage capacity of the battery. Cost function:\nCost of the energy bought on the network… minus the cost of the energy sold on the network. The problem is modeled by: Horizon: 24 hours, stepsize: 1 hour.\nOptimization over $T= 24$ intervals.\nOptimisation variable :\n$x(s)$ : state of charge of the battery at time $s$, $s= 0,…,T$ $a(s)$: amount of electricity bought on the network ($s= 0,…,T-1$). $v(s)$: amount of energy sold on the network ($s= 0,…,T-1$). Parameters:\n$d(s)$: net demand of energy (load minus solar production) at time $s$, $s= 0,…,T-1$. $P_a(s)$ : unitary buying price of energy at time $s$, $s= 0,…,T-1$ $P_v(s)$ : unitary selling price of energy at time $s$, $s= 0,…,T-1$ $x_{\\max}$: storage capacity of the battery. Contraints:\n$x(s+1)= x(s) - d(s) + a(s) - v(s)$, $\\forall s= 0,…,T-1$ $x(0)= 0$ $a(s) \\geq 0$, $\\forall s=0,…,T-1$ $v(s) \\geq 0$, $\\forall s=0,…,T-1$ $0 \\leq x(s) \\leq x_{\\max}$, $\\forall s=0,…T$. Cost function to be minimized: $$J(x,a,v)= \\sum_{s=0}^{T-1} \\Big( P_a(s) a(s) - P_v(s) v(s) \\Big).$$\nExercice 1 Write the optimization problem in a form that is compatible with the function linprog\n% Exercise 1: Indication: you should find optimal value= 124\nT= 24; # Time x_max= 5; # battery maximum storage P_a= np.array([2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 3, 3, 2]); # unitary buying price of energy at time s P_v= np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1]); # unitary selling price of energy at time s d =np.array([-1, -1, -1, -1, -1, -2, 2 ,6, 8, 4, 2, 0, 0, -1, -2, -1, 0, 1, 3, 7, 9, 3, 2, 0]); # net demand of energy (already known) #d =np.array([-2, -2, -2, -2, -2, -3, 1 ,4 ,6 ,3 ,1, -1, -1, -2, -3, -2, -1, 0, 2 ,5 ,5, 2, 1, -1]); #d =np.array([-3 ,-3 ,-3 ,-3 ,-3 ,-4 ,0 ,2 ,4, 2, 0 ,-2 ,-2 ,-3 ,-4 ,-3 ,-2 ,-1 ,1 ,3 ,5 ,1 ,0 ,-2]); d=d.reshape((T,1)) P_a=P_a.reshape(T,1) P_v=P_v.reshape(T,1) # with T=2 (for testing purposes ...) # T = 2; #x_max= 5; #P_a=np.array([-1, -1]) #P_v=P_a #d =np.array([-1, -1]) #d=d.reshape(T,1) #P_a=P_a.reshape(T,1) #P_v=P_v.reshape(T,1) def solve_deterministic_pb(): global d, P_a,P_v \u0026#34;\u0026#34;\u0026#34; print x,a,v and J(x,a,v) \u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34; ####### Inequality constraints: ####### xi\u0026lt;=xmax for i = 0.. T: concatenate A1=Identity matrix \u0026amp; A2 \u0026amp; A2 --------------- -x\u0026lt;=0 (size T+1 ! ) -a\u0026lt;=0 (size T) -v\u0026lt;=0 (size T) : identity matrix A3 (size 3T+1) --------------- #A X \u0026lt;= B Indications: with A of shape((len(x)+1 + len(a)+1 + len(v)+1 , len(x) + len (a) + len(v))) #e.g. (10, 7) for T=2 with B= [x_max*np.ones(T+1,1)], np.zeros((3*T+1,1))]; \u0026#34;\u0026#34;\u0026#34; # xi\u0026lt;=xmax for i = 0.. T: concatenate A1=Identity matrix \u0026amp; A2 \u0026amp; A2 A1=... A2=... A=np.concatenate((A1,A2,A2), axis=1) # -x, -a, -v \u0026lt;= 0,0,0 A3=... A=... #print(np.shape(A)) # xi\u0026lt;=xmax for i = 0.. T B1=... # -x, -a, -v \u0026lt;= 0,0,0 B3=... B=np.concatenate((B1.transpose(),B3), axis=0); #print(B) \u0026#34;\u0026#34;\u0026#34; ####### Equality constraints: ####### --------------- Aeq1: x(s+1)-x(s)-a(s)+v(s)=-d(s) for all s=0...T-1 -1. 1. on the diagonal (-x(s)+x(s+1)) -1 for a(s) 1 for v(s) --------------- Aeq2 x(0)=0 --------------- # Aeq X = Beq with Beq= [-d,0] # -d for Aeq1 and 0 for Aeq2 \u0026#34;\u0026#34;\u0026#34; # Aeq1: M1=... #-x(s) and x(s+1) s=0, T-1 M2=... # M=np.concatenate((-M1,M2), axis=1)+np.concatenate((M2,M1),axis=1) Aeq1=... # M \u0026amp; -a(s) \u0026amp; v(s) s=0, T-1 Aeq2=... # #x(0)=0, (with concatenate, don\u0026#39;t use axis=1 for only one line, use double parentheses) #print(Aeq2) Aeq=np.concatenate((Aeq1,Aeq2.transpose()),axis=0) #print(Aeq) Beq=np.concatenate((-d,np.zeros((1,1)))) #print(Beq) \u0026#34;\u0026#34;\u0026#34; solving min c^T * X s.t. A X \u0026lt;= B and Aeq X= Beq no x in min: 0; a P_a -v P_v \u0026#34;\u0026#34;\u0026#34; # min c^T [x,a,v] c=...#cost function: sum_s P_a a(s) - P_v(v) #print(c) #columns of A must be equal to size of c X=optimize.linprog(...) print(\u0026#34;x:\u0026#34;, X.x[:T+1]) print(np.shape(X.x[:T+1])) print(\u0026#34;a:\u0026#34;, X.x[T+1:2*T+1]) print(np.shape(X.x[T+1:2*T+1])) print(\u0026#34;v:\u0026#34;, X.x[2*T+1:]) print(np.shape(X.x[2*T+1:])) print(\u0026#34;Optimal cost\u0026#34;, X.fun) solve_deterministic_pb() x: [-0. 0. 0. 1. 2. 3. 5. 5. 0. 0. 0. 0. 0. 0. 1. 3. 5. 5. 5. 5. 0. 0. 0. 0. 0.] (25,) a: [ 0. 0. 0. 0. 0. 0. 2. 1. 8. 4. 2. -0. -0. 0. 0. 1. -0. 1. 3. 2. 9. 3. 2. 0.] (24,) v: [ 1. 1. -0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. -0.] (24,) Optimal cost 124.0 Exercice 2 interpolation d’ordre 2 % with y=[0 1 2] and val=[1 3 7], the solution is alpha= [1 1 1]\nfrom scipy.optimize import minimize def interpolate_2(y,val): def objective(alpha): return ... # initial guess n = 3 alpha0 = …","date":1734385080,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734385080,"objectID":"1fdf557ff2a296b51ebdbfd9a40ef142","permalink":"https://grosjean1.github.io/post/notebookdynprog/","publishdate":"2024-12-16T22:38:00+01:00","relpermalink":"/post/notebookdynprog/","section":"post","summary":"#Install package import sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib # import packages import numpy as np import scipy from scipy import optimize from scipy.optimize import minimize Deterministic model Time scale: the decisions are taken every hour during the day.","tags":[],"title":"TP1","type":"post"},{"authors":[],"categories":[],"content":" Project Logistic regression #Install package import sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib !{sys.executable} -m pip install scikit-learn import numpy as np import matplotlib.pyplot as plt from time import time import random import sklearn 1) Device Type: Choose a device type (Lights, Camera, Smart Speaker, Thermostat,…) and plot the data\n#import kagglehub #if from kagglehub #path = kagglehub.dataset_download(\u0026#34;rabieelkharoua/predict-smart-home-device-efficiency-dataset\u0026#34;) from csv import * filename = f\u0026#34;smart_home_device_usage_data.csv\u0026#34; #open file with open(filename, mode=\u0026#39;r\u0026#39;) as file: reader = reader(file) data = list(reader) # Convert in list to make all lines accessible n=len(data)-1 #number of lines DeviceType=\u0026#39;...\u0026#39; count = sum(1 for row in data if len(row) \u0026gt; 1 and row[1] == DeviceType) print(f\u0026#34;Nombre d\u0026#39;occurrences de l\u0026#39;objet choisi dans la colonne 2 : {count}\u0026#34;) xi_list=np.zeros((count,3)) # three parameters yi_list=np.zeros(count) #labels i=0 for row in data[1:]: if row[1]==DeviceType: yi_list[i]=row[7] # label: 0 - Inefficient, 1 - Efficient xi_list[i,0]=row[2] #usagehoursperday xi_list[i,1]=row[3] #energyconsumption xi_list[i,2]=row[6] #DeviceAgeMonths i+=1 assert(i==count) n=i #data number=count # normalize data (features between [0,1]): min_values = np.array([0, 0, 0]) # Features minimum max_values = np.array([25,10, 60]) # Features maximum x_normalized = (xi_list - min_values) / (max_values - min_values) xi_list=x_normalized #print(np.shape(x_normalized)) Nombre d\u0026#39;occurrences de l\u0026#39;objet choisi dans la colonne 2 : 1039 We are given a family of $n$ points $x_i=(x^0_i,x^1_i,x^2_i)$ in $\\mathbb{R}^3$, along with associated $\\textit{labels}$ $(y_i)_{1 \\leq i \\leq n}$ in ${0,1}$.\nIn previous work, we aimed to establish a relationship between $x$ and $y$ in the form:\nif $\\sigma(\\langle w, x\\rangle) \\geq 0.5$, then $y=1$, else $y=0$, where $w = (w^0, w^1)\\in \\mathbb{R}^2$ was the unknown of the problem and $\\sigma$ was the sigmoid function $z \\mapsto \\tfrac{1}{1+e^{-z}}$. Then, for a new unlabeled data $x$, we were computing $\\sigma(\\langle w,x\\rangle) \\in [0,1]$, allowing us to classify it as either $y=0$ or $y=1$ while incorporating a measure of uncertainty in the prediction.\nTo simplify the model, we assumed that the data were centered at $0$ and set the bias term $b=0$: we only had to minimize the log-loss function with respect to the weights.\nIn general, a bias term $b$ is required, meaning that we introduce an additional parameter $b \\in \\mathbb{R}$ and seek a relationship of the form:\nif $\\sigma(\\langle w, x\\rangle + b) \\geq 0.5$, then $y=1$, else $y=0$, since there is no reason to assume that the separating hyperplane should pass through the origin.\nHowever, we can eliminate the bias term by increasing the dimensionality from $d$ to $d + 1$. In our case, this means increasing the dimensions of both $x$ and $w$ from $3$ to $4$: we redefine $$x=(x^0,x^1,x^2,1) \\textrm{ and } w=(w^0,w^1,w^2,b).$$ This transformation allows us to maintain the same matrix notation as before in the optimization step.\nThe log-loss function without a penalization term is thus given by: \\begin{equation*} f(w)=-\\frac1n \\sum_{i=1}^n (y_i log (\\sigma(\\langle w,x_i \\rangle))+ (1-y_i)log (1-\\sigma(\\langle w,x_i \\rangle)). \\end{equation*}\nData plot # data plot from mpl_toolkits.mplot3d import Axes3D xi_list_bias=np.zeros((n,4)) #increasing dimension from 3 to 4 for i in range(n): #redefining data x in x=(x0,x1,x2,1) xi_list_bias[i, 0:3]= ... xi_list_bias[i, 3]= ... fig = plt.figure() ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) for i in range(n//2): if yi_list[i] == 1: # if x_i is of class 1 the point is red : 1 - Efficient ax.scatter(xi_list[i, 0], xi_list[i, 1], xi_list[i, 2], color=\u0026#34;red\u0026#34;, marker=\u0026#34;.\u0026#34;) else: # else green #0 - Inefficient ax.scatter(xi_list[i, 0], xi_list[i, 1], xi_list[i, 2], color=\u0026#34;green\u0026#34;, marker=\u0026#34;.\u0026#34;) elevation_angle = 60 azimuthal_angle = 15 ax.view_init(elevation_angle, azimuthal_angle) ax.set_xlabel(\u0026#34;X axis, usage hours/day\u0026#34;) ax.set_ylabel(\u0026#34;Y axis, device age\u0026#34;) ax.set_zlabel(\u0026#34;Z axis, energy consumption\u0026#34;) plt.title(u\u0026#34;Data representation.\\n\u0026#34; + r\u0026#34;$x_i$ is a green point if $y_i = 0$, red point if $y_i = 1$.\u0026#34;+u\u0026#34;\\n\u0026#34;+ r\u0026#34;0 - Inefficient, 1 - Efficient\u0026#34;) #plt.axis(\u0026#34;tight\u0026#34;) plt.show() 2. Log-loss function Define a log-loss function with penalty term L2 or with a barrier function.\n... 3. Gradient-descent algorithm: Write a gradient-descent algorithm with the Armijo or the Wolfe-Armijo rule.\n# gradient ... w=np.random.rand(4) epsilon = 1e-8 print(\u0026#34;This quantity must be of order 1e-8 : \u0026#34;, np.linalg.norm(gradf(w) - np.array([(f(w + epsilon*np.eye(4)[0]) - f(w))/(epsilon), (f(w + epsilon*np.eye(4)[1]) - f(w))/(epsilon),(f(w + epsilon*np.eye(4)[2]) - f(w))/(epsilon),(f(w + epsilon*np.eye(4)[3]) - f(w))/(epsilon)]))) This quantity must be of order 1e-8 : 3.603214732619758e-08 # Gradient descent ... Best weights/bias %%time x0 = np.array([1.,1.,1.,1.]) ... xlist_approx = ... …","date":1734081560,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734081560,"objectID":"5834c92a138ee91822b4d2d3631c35cf","permalink":"https://grosjean1.github.io/post/project/","publishdate":"2024-12-13T10:19:20+01:00","relpermalink":"/post/project/","section":"post","summary":"Project Logistic regression #Install package import sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib !{sys.executable} -m pip install scikit-learn import numpy as np import matplotlib.pyplot as plt from time import time import random import sklearn 1) Device Type: Choose a device type (Lights, Camera, Smart Speaker, Thermostat,…) and plot the data","tags":[],"title":"ENT306","type":"post"},{"authors":[],"categories":[],"content":"","date":1734081560,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734081560,"objectID":"abd19ff3197d55acdbe44dc93127f27d","permalink":"https://grosjean1.github.io/post/my-post/","publishdate":"2024-12-13T10:19:20+01:00","relpermalink":"/post/my-post/","section":"post","summary":"","tags":[],"title":"My Post","type":"post"},{"authors":[],"categories":[],"content":" TP 4 Logistic regression import sys !{sys.executable} -m pip install numdifftools !{sys.executable} -m pip install scikit-learn import numpy as np import matplotlib.pyplot as plt from time import time import random #params = {\u0026#39;tex.usetex\u0026#39;: True} We are given a family of $n$ points $x_i$ in $\\mathbb{R}^2$, as well as $n$ \\textit{labels} associated $(y_i)_{1 \\leq i \\leq n}$ in ${0,1}$.\nWe seek to find a relation between $x$ and $y$, in the form:\nif $\\sigma(\\langle w, x\\rangle) \\geq 0.5$, then $y=1$ else $y=0$, where $w = (w_1, w_2)\\in \\mathbb{R}^2$ is the unknown of the problem and $\\sigma$ is the sigmoid function $z \\mapsto \\tfrac{1}{1+e^{-z}}$.\nThus, for a new unlabeled data $x$, we will calculate $\\sigma(\\langle w,x\\rangle) \\in [0,1]$, which will allow us to classify the point into $y = 0$ or $y=1$, and this, with a form of taking into account the uncertainty associated with this prediction.\nTo do so, we have to minimize the log-loss function:\n\\begin{equation*} f(w)=-\\frac1n \\sum_{i=1}^n (y_i log (\\sigma(\\langle w,x_i \\rangle))+ (1-y_i)log (1-\\sigma(\\langle w,x_i \\rangle)) + \\lambda \\frac12 \\lVert w\\rVert^2, \\end{equation*} where $\\lambda$ is a regularization parameter.\nUseful functions # To observe the level circles of a function obj on a domain in R^2 def level_circle(obj, dom): # Grids definition and evaluation of f grid_size = 50 x, y = np.linspace(dom[0], dom[1], grid_size), np.linspace(dom[2], dom[3], grid_size) X, Y = np.meshgrid(x, y) Z = np.zeros((grid_size, grid_size)) for i in range(grid_size): for j in range(grid_size): Z[i, j] = obj([X[i, j], Y[i, j]]) fig, ax = plt.subplots(figsize = (10, 10)) CS = ax.contour(X, Y, Z, 50) ax.clabel(CS, fontsize = 8) ax.set_aspect(\u0026#39;equal\u0026#39;, adjustable = \u0026#39;box\u0026#39;) # pour que les axes aient la même échelle ax.set_xlim([dom[0], dom[1]]) ax.set_ylim([dom[2], dom[3]]) return fig, ax # Function that allows to visualize the level circles of a function (obj), a list of points (xlist), and a final point (xstar) # The parameter m allows to plot the points only each m values # i.e. we display x_{m j} for all j def plot_descent(obj, dom, m, xlist, xstar): grid_size = 50 X, Y = np.linspace(dom[0], dom[1], grid_size), np.linspace(dom[2], dom[3], grid_size) X, Y = np.meshgrid(X, Y) Z = np.zeros((grid_size, grid_size)) for i in range(grid_size): for j in range(grid_size): Z[i, j] = obj([X[i, j], Y[i, j]]) fig, ax = plt.subplots(figsize = (10, 10)) CS = ax.contour(X, Y, Z, 100) ax.clabel(CS, fontsize = 10) ax.set_xlim(dom[0], dom[1]) ax.set_ylim(dom[2], dom[3]) xlist = xlist[0::m] Xlist = [xlist[i][0] for i in range(len(xlist))] Ylist = [xlist[i][1] for i in range(len(xlist))] plt.plot(xstar[0], xstar[1], marker = \u0026#34;*\u0026#34;, ms = 10.0, color = \u0026#34;r\u0026#34;) plt.plot(Xlist, Ylist, marker = \u0026#34;o\u0026#34;, ms = 4, color = \u0026#34;red\u0026#34;) Data generation and plot # Parameter useful to generate the data t = 2*np.pi*np.random.rand() u = np.array([np.cos(t), np.sin(t)]) def genererCouple(): if np.random.rand() \u0026lt; 0.5: x, y = 2.*np.random.randn(2) + 3*u, 0 # green else: x, y = 2.*np.random.randn(2) - 3*u, 1 # red return [x, y] # data generation n = 200 # n points xi_list, yi_list = np.zeros((n, 2)), np.zeros(n) for i in range(n): xi_list[i, :], yi_list[i] = genererCouple() # we stock the data ((x_i, y_i))_i # plot plt.figure(figsize = (5, 5)) for i in range(n): if yi_list[i] == 1: # if x_i is of class 1 the point is red plt.plot(xi_list[i, 0], xi_list[i, 1], linestyle = \u0026#34;none\u0026#34;, marker = \u0026#39;.\u0026#39;, color = \u0026#34;red\u0026#34;) else: # else green plt.plot(xi_list[i, 0], xi_list[i, 1], linestyle = \u0026#34;none\u0026#34;, marker = \u0026#39;.\u0026#39;, color = \u0026#34;green\u0026#34;) plt.title(u\u0026#34;Data representation.\\n\u0026#34; + r\u0026#34;$x_i$ is a green point if $y_i = 0$, red point if $y_i = 1$.\u0026#34;) plt.axis(\u0026#34;tight\u0026#34;) plt.show() Exercice 1: Write one function $\\texttt{f(w)}$ for the log-loss $f$ and one function $\\texttt{gradf(w)}$ which compute its gradient.\nIndication: You can use the facts that $\\sigma’(z)=\\sigma(z) (1-\\sigma(z))$ and that $1-\\sigma(z)=\\sigma(-z)$ and write the derivatives of two functions $\\texttt{phi1}$ and $\\texttt{phi2}$ that will represents $- log (\\sigma(z))$ and $-log(1-\\sigma(z))$ resp.\n# Choice for the regularization parameter lda = 1e-2#.01 # Sigmoid function def sigmoid(t): return ... def f(w): ... ## Level circles of f dom = ... fig, ax = level_circle(f,dom) def phi1_deriv(t): return ... def phi2_deriv(t): return ... # Note : il est possible de définir une version vectorisée du gradient (sans boucle for), # que je n\u0026#39;ai pas implémenté ici. Le calcul en serait significativement accéléré. def gradf(w): ... # Check that your gradient implementation is correct w = np.random.rand(2) epsilon = 1e-8 print(\u0026#34;This quantity must be of order 1e-8 : \u0026#34;, np.linalg.norm(gradf(w) - np.array([(f(w + epsilon*np.array([1,0])) - f(w))/(epsilon), (f(w + epsilon*np.array([0,1])) - f(w))/(epsilon)]))) This quantity must be of order 1e-8 : 1.4343463764364958e-07 Exercice 2: Code the gradient_descent (with a fixed step, with Armijo and Armijo_Wolfe rules). You can use your previous TP on …","date":1734081560,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734081560,"objectID":"49c6b774aa043a1ebdf065d4c15abcc4","permalink":"https://grosjean1.github.io/post/tp5/","publishdate":"2024-12-13T10:19:20+01:00","relpermalink":"/post/tp5/","section":"post","summary":"TP 4 Logistic regression import sys !{sys.executable} -m pip install numdifftools !{sys.executable} -m pip install scikit-learn import numpy as np import matplotlib.pyplot as plt from time import time import random #params = {'tex.","tags":[],"title":"TP 2 (End)","type":"post"},{"authors":[],"categories":[],"content":"#Install package import sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib # import packages import numpy as np import scipy from scipy import optimize from scipy.optimize import minimize Deterministic model Time scale: the decisions are taken every hour during the day.\nOptimization variables (at each time step) :\nthe amount of energy to be stored or withdrawn from the battery the amount of energy bought or sold to the network. Contraints:\nnonnegativity of variables evolution of the battery storage capacity of the battery. Cost function:\nCost of the energy bought on the network… minus the cost of the energy sold on the network. The problem is modeled by: Horizon: 24 hours, stepsize: 1 hour.\nOptimization over $T= 24$ intervals.\nOptimisation variable :\n$x(s)$ : state of charge of the battery at time $s$, $s= 0,…,T$ $a(s)$: amount of electricity bought on the network ($s= 0,…,T-1$). $v(s)$: amount of energy sold on the network ($s= 0,…,T-1$). Parameters:\n$d(s)$: net demand of energy (load minus solar production) at time $s$, $s= 0,…,T-1$. $P_a(s)$ : unitary buying price of energy at time $s$, $s= 0,…,T-1$ $P_v(s)$ : unitary selling price of energy at time $s$, $s= 0,…,T-1$ $x_{\\max}$: storage capacity of the battery. Contraints:\n$x(s+1)= x(s) - d(s) + a(s) - v(s)$, $\\forall s= 0,…,T-1$ $x(0)= 0$ $a(s) \\geq 0$, $\\forall s=0,…,T-1$ $v(s) \\geq 0$, $\\forall s=0,…,T-1$ $0 \\leq x(s) \\leq x_{\\max}$, $\\forall s=0,…T$. Cost function to be minimized: $$J(x,a,v)= \\sum_{s=0}^{T-1} \\Big( P_a(s) a(s) - P_v(s) v(s) \\Big).$$\nWe call demand scenario a vector $(D(s))_{s=1,…,T}$.\nTwo set of scenarios are available:\nTraining set $D_T$ : history of $NT$ demand scenarios. Used to build a probabilistic model for the demand and an appropriate control strategy. Simulation set $D_S$ : history of $NS$ demand scenarios. Used to test the control strategies. Avoid to build biased strategies. Shifting of the time index:\nThe two available histories of demand scenarios contain $T_0$ values of the demand from the “previous day”, corresponding to the time intervals $-T0, −(T0 − 1),…,-2,-1$. A demand scenario is a vector of size $T + T0$. The training and simulation sets are matrices with $(T + T0)$ columns and respectively $NT$ and $NS$ rows. We “get access” to the demand at time $t$, for the scenario $l$ with $DT(l,t +T0)$ or $DS(l,t +T0)$.\nOptimization problem in a form that is compatible with the function linprog % Exercise 1: Indication: you should find optimal value= 124 with $d$ a-priori known\nwe changed the function to take $d$ as an input parameter\nT= 24; # Time T0=10 #Training times x_max= 10; # battery maximum storage P_a= np.array([2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 3, 3, 2]); # unitary buying price of energy at time s P_v= np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1]); # unitary selling price of energy at time s #d =np.array([-1, -1, -1, -1, -1, -2, 2 ,6, 8, 4, 2, 0, 0, -1, -2, -1, 0, 1, 3, 7, 9, 3, 2, 0]); # net demand of energy (already known) #d=d.reshape((T,1)) P_a=P_a.reshape(T,1) P_v=P_v.reshape(T,1) NT=200 NS=200 import scipy.io mat = scipy.io.loadmat(\u0026#39;scenarios.mat\u0026#39;) DS = mat[\u0026#39;D_S\u0026#39;] DT = mat[\u0026#39;D_T\u0026#39;] # with T=2 (for testing purposes ...) # T = 2; #x_max= 5; #P_a=np.array([-1, -1]) #P_v=P_a #d =np.array([-1, -1]) #d=d.reshape(T,1) #P_a=P_a.reshape(T,1) #P_v=P_v.reshape(T,1) #same as exercice 1 def solve_deterministic_pb(d): ... c=np.concatenate((np.zeros((T+1,1)),P_a,-P_v),axis=0) #cost function: sum_s P_a a(s) - P_v(v) #print(c) #columns of A must be equal to size of c X=optimize.linprog(c,A_ub=A,b_ub=B,A_eq=Aeq,b_eq=Beq) x=X.x[:T+1] a=X.x[T+1:2*T+1] v=X.x[2*T+1:] #print(\u0026#34;x:\u0026#34;, X.x[:T+1]) #print(np.shape(X.x[:T+1])) #print(\u0026#34;a:\u0026#34;, X.x[T+1:2*T+1]) #print(np.shape(X.x[T+1:2*T+1])) #print(\u0026#34;v:\u0026#34;, X.x[2*T+1:]) #print(np.shape(X.x[2*T+1:])) #print(\u0026#34;Optimal cost\u0026#34;, X.fun) return x,a,v,X.fun Exercice 6 compute $J_{anti}$, the optimal cost Given a scenario $D$, compute $J_{anti}=\\frac1{NS} \\sum_{l=1}^{NS} J_{anti}(DS(l,.))$ , where $J_{anti}(DS(l,.))= \\sum_{s=0}^{T-1} \\Big( P_a(s) a(s) - P_v(s) v(s) \\Big).$\nexpected result: J_eval= 10.2490\ndef lower_bound(): global T0,DS,NS J= 0; for l in range(0,NS):#=0:N_S-1 _, _, _, val = ... #only simulation D_S (pay attention to time shifting!) J= ... #print(\u0026#34;val\u0026#34;,val) J= ... return J #% Exercise 6: J=lower_bound() print(J) 10.249021164754385 Exercice 7 The naive strategy (we don’t exploit the training set: $\\mathcal{I}=0$) online step: at time $s$, given $d(s)$, we choose $$(a(s),v(s))= \\begin{align*} \u0026amp; (d(s),0) \\textrm{ if } d(s) \\geq 0,\\ \u0026amp; (0,-d(s)) \\textrm{ otherwise.} \\end{align*}$$\nInput: Demand scenario\ndef naive_online(D): global T; global P_a; global P_v; J=0; for s in range(T): if ... a=... v=... else: a=... v=... J= ... return J #% Exercise 7: def naive_eval(): global DS,NS,T0; J=0 for l in range(NS): J=... J= ... return J J=naive_eval() print(J) # the solution is 52.8324 NS 200 (200, 34) 10 …","date":1734081560,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734081560,"objectID":"e614139946ffcc9d2c4f2d3e7984a507","permalink":"https://grosjean1.github.io/post/tp3ent306/","publishdate":"2024-12-13T10:19:20+01:00","relpermalink":"/post/tp3ent306/","section":"post","summary":"#Install package import sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib # import packages import numpy as np import scipy from scipy import optimize from scipy.optimize import minimize Deterministic model Time scale: the decisions are taken every hour during the day.","tags":[],"title":"TP 3","type":"post"},{"authors":[],"categories":[],"content":"#Install package import sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib # import packages import numpy as np import scipy from scipy import optimize from scipy.optimize import minimize Deterministic model Time scale: the decisions are taken every hour during the day.\nOptimization variables (at each time step) :\nthe amount of energy to be stored or withdrawn from the battery the amount of energy bought or sold to the network. Contraints:\nnonnegativity of variables evolution of the battery storage capacity of the battery. Cost function:\nCost of the energy bought on the network… minus the cost of the energy sold on the network. The problem is modeled by: Horizon: 24 hours, stepsize: 1 hour.\nOptimization over $T= 24$ intervals.\nOptimisation variable :\n$x(s)$ : state of charge of the battery at time $s$, $s= 0,…,T$ $a(s)$: amount of electricity bought on the network ($s= 0,…,T-1$). $v(s)$: amount of energy sold on the network ($s= 0,…,T-1$). Parameters:\n$d(s)$: net demand of energy (load minus solar production) at time $s$, $s= 0,…,T-1$. $P_a(s)$ : unitary buying price of energy at time $s$, $s= 0,…,T-1$ $P_v(s)$ : unitary selling price of energy at time $s$, $s= 0,…,T-1$ $x_{\\max}$: storage capacity of the battery. Contraints:\n$x(s+1)= x(s) - d(s) + a(s) - v(s)$, $\\forall s= 0,…,T-1$ $x(0)= 0$ $a(s) \\geq 0$, $\\forall s=0,…,T-1$ $v(s) \\geq 0$, $\\forall s=0,…,T-1$ $0 \\leq x(s) \\leq x_{\\max}$, $\\forall s=0,…T$. Cost function to be minimized: $$J(x,a,v)= \\sum_{s=0}^{T-1} \\Big( P_a(s) a(s) - P_v(s) v(s) \\Big).$$\nWe call demand scenario a vector $(D(s))_{s=1,…,T}$.\nTwo set of scenarios are available:\nTraining set $D_T$ : history of $NT$ demand scenarios. Used to build a probabilistic model for the demand and an appropriate control strategy. Simulation set $D_S$ : history of $NS$ demand scenarios. Used to test the control strategies. Avoid to build biased strategies. Shifting of the time index:\nThe two available histories of demand scenarios contain $T_0$ values of the demand from the “previous day”, corresponding to the time intervals $-T0, −(T0 − 1),…,-2,-1$. A demand scenario is a vector of size $T + T0$. The training and simulation sets are matrices with $(T + T0)$ columns and respectively $NT$ and $NS$ rows. We “get access” to the demand at time $t$, for the scenario $l$ with $DT(l,t +T0)$ or $DS(l,t +T0)$.\nOptimization problem in a form that is compatible with the function linprog % Exercise 1: Indication: you should find optimal value= 124 with $d$ a-priori known\nwe changed the function to take $d$ as an input parameter\nT= 24; # Time T0=10 #Training times x_max= 10; # battery maximum storage P_a= np.array([2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 3, 3, 2]); # unitary buying price of energy at time s P_v= np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1]); # unitary selling price of energy at time s #d =np.array([-1, -1, -1, -1, -1, -2, 2 ,6, 8, 4, 2, 0, 0, -1, -2, -1, 0, 1, 3, 7, 9, 3, 2, 0]); # net demand of energy (already known) #d=d.reshape((T,1)) P_a=P_a.reshape(T,1) P_v=P_v.reshape(T,1) NT=200 NS=200 import scipy.io mat = scipy.io.loadmat(\u0026#39;scenarios.mat\u0026#39;) DS = mat[\u0026#39;D_S\u0026#39;] DT = mat[\u0026#39;D_T\u0026#39;] # with T=2 (for testing purposes ...) # T = 2; #x_max= 5; #P_a=np.array([-1, -1]) #P_v=P_a #d =np.array([-1, -1]) #d=d.reshape(T,1) #P_a=P_a.reshape(T,1) #P_v=P_v.reshape(T,1) #same as exercice 1 def solve_deterministic_pb(d): ... c=np.concatenate((np.zeros((T+1,1)),P_a,-P_v),axis=0) #cost function: sum_s P_a a(s) - P_v(v) #print(c) #columns of A must be equal to size of c X=optimize.linprog(c,A_ub=A,b_ub=B,A_eq=Aeq,b_eq=Beq) x=X.x[:T+1] a=X.x[T+1:2*T+1] v=X.x[2*T+1:] #print(\u0026#34;x:\u0026#34;, X.x[:T+1]) #print(np.shape(X.x[:T+1])) #print(\u0026#34;a:\u0026#34;, X.x[T+1:2*T+1]) #print(np.shape(X.x[T+1:2*T+1])) #print(\u0026#34;v:\u0026#34;, X.x[2*T+1:]) #print(np.shape(X.x[2*T+1:])) #print(\u0026#34;Optimal cost\u0026#34;, X.fun) return x,a,v,X.fun Exercice 6 compute $J_{anti}$, the optimal cost Given a scenario $D$, compute $J_{anti}=\\frac1{NS} \\sum_{l=1}^{NS} J_{anti}(DS(l,.))$ , where $J_{anti}(DS(l,.))= \\sum_{s=0}^{T-1} \\Big( P_a(s) a(s) - P_v(s) v(s) \\Big).$\nexpected result: J_eval= 10.2490\ndef lower_bound(): global T0,DS,NS J= 0; for l in range(0,NS):#=0:N_S-1 _, _, _, val = ... #only simulation D_S (pay attention to time shifting!) J= ... #print(\u0026#34;val\u0026#34;,val) J= ... return J #% Exercise 6: J=lower_bound() print(J) 10.249021164754385 Exercice 7 The naive strategy (we don’t exploit the training set: $\\mathcal{I}=0$) online step: at time $s$, given $d(s)$, we choose $$(a(s),v(s))= \\begin{align*} \u0026amp; (d(s),0) \\textrm{ if } d(s) \\geq 0,\\ \u0026amp; (0,-d(s)) \\textrm{ otherwise.} \\end{align*}$$\nInput: Demand scenario\ndef naive_online(D): global T; global P_a; global P_v; J=0; for s in range(T): if ... a=... v=... else: a=... v=... J= ... return J #% Exercise 7: def naive_eval(): global DS,NS,T0; J=0 for l in range(NS): J=... J= ... return J J=naive_eval() print(J) # the solution is 52.8324 NS 200 (200, 34) 10 …","date":1734081560,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734081560,"objectID":"7fee120cbc470ab52c06a8b4521110f1","permalink":"https://grosjean1.github.io/post/tp2ent306/","publishdate":"2024-12-13T10:19:20+01:00","relpermalink":"/post/tp2ent306/","section":"post","summary":"#Install package import sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib # import packages import numpy as np import scipy from scipy import optimize from scipy.optimize import minimize Deterministic model Time scale: the decisions are taken every hour during the day.","tags":[],"title":"Tp2","type":"post"},{"authors":[],"categories":[],"content":" TP 4 Logistic regression import numpy as np import matplotlib.pyplot as plt from time import time import random #params = {\u0026#39;tex.usetex\u0026#39;: True} We are given a family of $n$ points $x_i$ in $\\mathbb{R}^2$, as well as $n$ \\textit{labels} associated $(y_i)_{1 \\leq i \\leq n}$ in ${0,1}$.\nWe seek to find a relation between $x$ and $y$, in the form:\nif $\\sigma(\\langle w, x\\rangle) \\geq 0.5$, then $y=1$ else $y=0$, where $w = (w_1, w_2)\\in \\mathbb{R}^2$ is the unknown of the problem and $\\sigma$ is the sigmoid function $z \\mapsto \\tfrac{1}{1+e^{-z}}$.\nThus, for a new unlabeled data $x$, we will calculate $\\sigma(\\langle w,x\\rangle) \\in [0,1]$, which will allow us to classify the point into $y = 0$ or $y=1$, and this, with a form of taking into account the uncertainty associated with this prediction.\nTo do so, we have to minimize the log-loss function:\n\\begin{equation*} f(w)=-\\frac1n \\sum_{i=1}^n (y_i log (\\sigma(\\langle w,x_i \\rangle))+ (1-y_i)log (1-\\sigma(\\langle w,x_i \\rangle)) + \\lambda \\frac12 \\lVert w\\rVert^2, \\end{equation*} where $\\lambda$ is a regularization parameter.\nUseful functions # To observe the level circles of a function obj on a domain in R^2 def level_circle(obj, dom): # Grids definition and evaluation of f grid_size = 50 x, y = np.linspace(dom[0], dom[1], grid_size), np.linspace(dom[2], dom[3], grid_size) X, Y = np.meshgrid(x, y) Z = np.zeros((grid_size, grid_size)) for i in range(grid_size): for j in range(grid_size): Z[i, j] = obj([X[i, j], Y[i, j]]) fig, ax = plt.subplots(figsize = (10, 10)) CS = ax.contour(X, Y, Z, 50) ax.clabel(CS, fontsize = 8) ax.set_aspect(\u0026#39;equal\u0026#39;, adjustable = \u0026#39;box\u0026#39;) # pour que les axes aient la même échelle ax.set_xlim([dom[0], dom[1]]) ax.set_ylim([dom[2], dom[3]]) return fig, ax # Function that allows to visualize the level circles of a function (obj), a list of points (xlist), and a final point (xstar) # The parameter m allows to plot the points only each m values # i.e. we display x_{m j} for all j def plot_descent(obj, dom, m, xlist, xstar): grid_size = 50 X, Y = np.linspace(dom[0], dom[1], grid_size), np.linspace(dom[2], dom[3], grid_size) X, Y = np.meshgrid(X, Y) Z = np.zeros((grid_size, grid_size)) for i in range(grid_size): for j in range(grid_size): Z[i, j] = obj([X[i, j], Y[i, j]]) fig, ax = plt.subplots(figsize = (10, 10)) CS = ax.contour(X, Y, Z, 100) ax.clabel(CS, fontsize = 10) ax.set_xlim(dom[0], dom[1]) ax.set_ylim(dom[2], dom[3]) xlist = xlist[0::m] Xlist = [xlist[i][0] for i in range(len(xlist))] Ylist = [xlist[i][1] for i in range(len(xlist))] plt.plot(xstar[0], xstar[1], marker = \u0026#34;*\u0026#34;, ms = 10.0, color = \u0026#34;r\u0026#34;) plt.plot(Xlist, Ylist, marker = \u0026#34;o\u0026#34;, ms = 4, color = \u0026#34;red\u0026#34;) Data generation and plot # Parameter useful to generate the data t = 2*np.pi*np.random.rand() u = np.array([np.cos(t), np.sin(t)]) def genererCouple(): if np.random.rand() \u0026lt; 0.5: x, y = 2.*np.random.randn(2) + 3*u, 0 # green else: x, y = 2.*np.random.randn(2) - 3*u, 1 # red return [x, y] # data generation n = 200 # n points xi_list, yi_list = np.zeros((n, 2)), np.zeros(n) for i in range(n): xi_list[i, :], yi_list[i] = genererCouple() # we stock the data ((x_i, y_i))_i # plot plt.figure(figsize = (5, 5)) for i in range(n): if yi_list[i] == 1: # if x_i is of class 1 the point is red plt.plot(xi_list[i, 0], xi_list[i, 1], linestyle = \u0026#34;none\u0026#34;, marker = \u0026#39;.\u0026#39;, color = \u0026#34;red\u0026#34;) else: # else green plt.plot(xi_list[i, 0], xi_list[i, 1], linestyle = \u0026#34;none\u0026#34;, marker = \u0026#39;.\u0026#39;, color = \u0026#34;green\u0026#34;) plt.title(u\u0026#34;Data representation.\\n\u0026#34; + r\u0026#34;$x_i$ is a green point if $y_i = 0$, red point if $y_i = 1$.\u0026#34;) plt.axis(\u0026#34;tight\u0026#34;) plt.show() Exercice 1: Write one function $\\texttt{f(w)}$ for the log-loss $f$ and one function $\\texttt{gradf(w)}$ which compute its gradient.\nIndication: You can use the facts that $\\sigma’(z)=\\sigma(z) (1-\\sigma(z))$ and that $1-\\sigma(z)=\\sigma(-z)$ and write the derivatives of two functions $\\texttt{phi1}$ and $\\texttt{phi2}$ that will represents $- log (\\sigma(z))$ and $-log(1-\\sigma(z))$ resp.\n# Choice for the regularization parameter lda = 1e-2#.01 # Sigmoid function def sigmoid(t): return ... def f(w): ... ## Level circles of f dom = ... fig, ax = level_circle(f,dom) def phi1_deriv(t): return ... def phi2_deriv(t): return ... # Note : il est possible de définir une version vectorisée du gradient (sans boucle for), # que je n\u0026#39;ai pas implémenté ici. Le calcul en serait significativement accéléré. def gradf(w): ... # Check that your gradient implementation is correct w = np.random.rand(2) epsilon = 1e-8 print(\u0026#34;This quantity must be of order 1e-8 : \u0026#34;, np.linalg.norm(gradf(w) - np.array([(f(w + epsilon*np.array([1,0])) - f(w))/(epsilon), (f(w + epsilon*np.array([0,1])) - f(w))/(epsilon)]))) This quantity must be of order 1e-8 : 1.4343463764364958e-07 Exercice 2: Code the gradient_descent (with a fixed step, with Armijo and Armijo_Wolfe rules). You can use your previous TP on gradient_descent (ENT305).\nFixed Step: $x=x+\\alpha d$ with $d=-\\nabla f(x)$\nArmijo:\n$\\Phi(\\alpha)=f(x+\\alpha d)$ …","date":1734081560,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734081560,"objectID":"05e6e36499d32c8ab8e3122ec96d39cc","permalink":"https://grosjean1.github.io/post/tp4ent306/","publishdate":"2024-12-13T10:19:20+01:00","relpermalink":"/post/tp4ent306/","section":"post","summary":"TP 4 Logistic regression import numpy as np import matplotlib.pyplot as plt from time import time import random #params = {'tex.usetex': True} We are given a family of $n$ points $x_i$ in $\\mathbb{R}^2$, as well as $n$ \\textit{labels} associated $(y_i)_{1 \\leq i \\leq n}$ in ${0,1}$.","tags":[],"title":"Tp4","type":"post"},{"authors":["Elise Grosjean"],"categories":null,"content":"","date":1733184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733184000,"objectID":"7ea47b283b61486520cb0c919ed248e2","permalink":"https://grosjean1.github.io/talk/idefix-days-2024/","publishdate":"2024-12-03T00:00:00Z","relpermalink":"/talk/idefix-days-2024/","section":"event","summary":"RBMs for you every day PDE life","tags":[],"title":"Idefix Days 2024","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":"","date":1730419200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730419200,"objectID":"a8096832b17da148ec475f67cf31d27d","permalink":"https://grosjean1.github.io/talk/mathematics-and-art-exploring-connections/","publishdate":"2024-11-01T00:00:00Z","relpermalink":"/talk/mathematics-and-art-exploring-connections/","section":"event","summary":"","tags":[],"title":"Mathematics and Art, exploring connections","type":"event"},{"authors":[],"categories":[],"content":"data; #incoming flow param :trange: Q:= 1 386 2 346 3 416 4 713 5 1532 6 2000 7 1982 8 1431 9 780 10 476 11 450 12 420 ; # revenue param B:= 1 3500 2 700 3 2250 4 500 ; param d : 1 2 3 4 5 6 7 8 9 10 11 12:= 1 3 3 3 2 4 5 5 4 5 0 0 0 2 0 0 0 1 3 3 0 0 0.2 0 0 0 3 0 0 0 4.5 7 8 8 7 0.5 0 0 0 4 0 0 0 0.5 2 3.5 3.5 2.5 0.5 0 0 0 ; model; ","date":1728975550,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728975550,"objectID":"f65e801e2d246e6bca5410e80db74226","permalink":"https://grosjean1.github.io/post/exo7/","publishdate":"2024-10-15T08:59:10+02:00","relpermalink":"/post/exo7/","section":"post","summary":"data; #incoming flow param :trange: Q:= 1 386 2 346 3 416 4 713 5 1532 6 2000 7 1982 8 1431 9 780 10 476 11 450 12 420 ; # revenue param B:= 1 3500 2 700 3 2250 4 500 ; param d : 1 2 3 4 5 6 7 8 9 10 11 12:= 1 3 3 3 2 4 5 5 4 5 0 0 0 2 0 0 0 1 3 3 0 0 0.","tags":[],"title":"Exo7","type":"post"},{"authors":[],"categories":[],"content":"data; param:nrange :t:= 1 1790 2 1800 3 1810 4 1820 5 1830 6 1840 7 1850 8 1860 9 1870 10 1880 11 1890 12 1900 13 1910 14 1920 15 1930 16 1940 17 1950 18 1960 19 1970 20 1980 21 1990 22 2000; param P:= 1 3.93 2 5.31 3 7.24 4 9.64 5 12.86 6 17.06 7 23.19 8 31.44 9 38.56 10 50.19 11 62.98 12 76.21 13 92.23 14 106.02 15 123.20 16 132.16 17 151.33 18 179.32 19 203.30 20 226.54 21 248.71 22 281.42 ; model; ","date":1728974892,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728974892,"objectID":"76cd5b3ec59936308ca7dcca65384378","permalink":"https://grosjean1.github.io/post/exo6/","publishdate":"2024-10-15T08:48:12+02:00","relpermalink":"/post/exo6/","section":"post","summary":"data; param:nrange :t:= 1 1790 2 1800 3 1810 4 1820 5 1830 6 1840 7 1850 8 1860 9 1870 10 1880 11 1890 12 1900 13 1910 14 1920 15 1930 16 1940 17 1950 18 1960 19 1970 20 1980 21 1990 22 2000; param P:= 1 3.","tags":[],"title":"Exo6","type":"post"},{"authors":[],"categories":[],"content":"Gradient descent with projection and augmented lagrangian algorithm To find the infimum of an arbitrary cost function, we use here the gradient descent. Let us consider the function $f(x,y)=0.5(x^2+(y-1)^2)$ that we aim at minimizing on $K\\subset \\mathbb{R}^2$ with $K={(x,y) | x= y}$. We start by manually compute its gradient and we will display its evolution during gradient descent.\nimport sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib 1. Complete the following code import numpy as np import matplotlib.pyplot as plt def f(x1,x2): # function return 0.5*(x1**2+(x2-1)**2) def df(x1,x2): # gradient return np.array([x1,x2-1]) def g(x1,x2): # constraint return -x1+x2 def dg(x1,x2): # constraint gradient return np.array([-1,1]) def L(x1,x2,mu): # lagrangian return f(x1,x2)+mu* (g(x1,x2)) def Lc(x1,x2,mu,c): # augmented lagrangian return f(x1,x2)+c/2.* np.dot(g(x1,x2),g(x1,x2))+mu* (g(x1,x2)) def dxLc(x1,x2,mu,c): # augmented lagrangian gradient return df(x1,x2)+ c* np.dot(g(x1,x2),dg(x1,x2)) +mu* (dg(x1,x2)) #(x1-x2) (1,-1) We need to define a norm to now how far from the global solution we are.\ndef norm(a): return np.sqrt(a[0]**2+a[1]**2) With $\\alpha$ too big, we found out that the solution blows up in the previous exercice, a correct choice for $\\alpha$ is very important. That is why, we will proceed with Armijo-Wolfe rule:\n# Wolfe rule: c1=.6 c2=.8 beta=1. #test with beta=1 eta=2. gamma=0.01 def phik(xyk,alphak,dk,fc,mu,c): return fc(xyk[0]+alphak*dk[0],xyk[1]+alphak*dk[1],mu,c) def dphik(xyk,alphak,dk,dfc,mu,c): grad_f=dfc(xyk[0]+alphak*dk[0],xyk[1]+alphak*dk[1],mu,c) return np.dot(grad_f,dk) def wolfe_rule(alpha,xy_old,d,fc,dfc,mu=0,c=0): #d_x est la direction de descente d_x . grad_x \u0026lt;= 0 # test f(x_new) \\leq f(x_0) + c alpha ps{d_x}{grad_x} test = 1 iteration = 0 min_ = 0 max_ = 1000 while (phik(xy_old,alpha,d,fc,mu,c)\u0026lt;=(phik(xy_old,0,d,fc,mu,c)+c1*dphik(xy_old,0,d,dfc,mu,c)*alpha)) \u0026amp;(iteration\u0026lt;=500): #armijo ok alpha=eta*alpha iteration = 0 while (test!=0)\u0026amp;(iteration\u0026lt;=500): xnew0,xnew1=xy_old[0]+alpha*d[0],xy_old[1]+alpha*d[1] if (phik(xy_old,alpha,d,fc,mu,c)\u0026lt;= (phik(xy_old,0,d,fc,mu,c)+c1*dphik(xy_old,0,d,dfc,mu,c)*alpha)) \u0026amp; (np.dot(dfc(xnew0,xnew1,mu,c),d) \u0026gt;= c2*np.dot(dfc(xy_old[0],xy_old[1],mu,c),d) ): test = 0 elif phik(xy_old,alpha,d,fc,mu,c)\u0026gt; (phik(xy_old,0,d,fc,mu,c)+c1*dphik(xy_old,0,d,dfc,mu,c)*alpha): #no armijo max_ = alpha alpha = (max_ + min_)/2 iteration = iteration +1 else: # armijo ok minorant = alpha alpha = (max_ + min_)/2 iteration = iteration +1 return alpha 2. Now we proceed with the Gradient descent algorithm with Armijo-Wolfe rule: Complete the following code def GradientDescent(fc,dfc,xy0,beta,mu=0,c=0): #initialization for Armijo alpha=beta # start GD algorithm eps=1e-6 x0,y0=xy0[0],xy0[1] #starting points evolution = [[x0, y0]] grad_f = dfc(x0, y0,mu=mu,c=c) #gradient function cpt_grad=0 while norm(grad_f)\u0026gt;=eps and cpt_grad\u0026lt;500: d = -grad_f #direction alpha=beta #armijo xy_old=np.array([x0,y0]) alpha=wolfe_rule(alpha,xy_old,d,fc,dfc, mu=mu,c=c) # find best alpha x0, y0 = x0 + alpha*d[0], y0 + alpha*d[1] evolution = np.vstack((evolution, [x0, y0])) grad_f = dfc(x0, y0,mu=mu,c=c) cpt_grad+=1 return evolution 3. Complete the code of the augmented lagrangian: #initialization for Armijo alpha=beta ## start GD algorithm eps=1e-6 # initial points x1,x2=1.,1. c=1. # penalty parameter mu=1.7 # dual variable gold=g(x1,x2) # not a golden function, but the constraints are very precious gradx_f = dxLc(x1, x2,mu=mu,c=c) # gradient of the augmented lagrangian cpt=0 # to avoid infinite while loop xy0=[x1,x2] while norm(gradx_f)\u0026gt;eps or np.abs(g(x1,x2))\u0026gt;eps : evolution=GradientDescent(Lc,dxLc,xy0,beta,mu=mu,c=c) evolution_X = evolution[:, 0] evolution_Y = evolution[:, 1] x1,x2=evolution_X[-1],evolution_Y[-1] # last points print(\u0026#34;x1:\u0026#34;, x1, \u0026#34; x2:\u0026#34;, x2) if (np.abs(g(x1,x2)))\u0026lt;np.abs(gold): # if ||g|| is small #print(\u0026#34;||g|| smaller\u0026#34;,np.abs(g(x1,x2))) mu=mu+c*g(x1,x2) #increase mu print(\u0026#34;new mu: \u0026#34;, mu) else: c=c*2 # increase c print(\u0026#34;new c: \u0026#34;,c) cpt+=1 gold=g(x1,x2) #updates gradx_f = dxLc(x1, x2,mu,c) print(\u0026#34;norm of gradx_f: \u0026#34;,norm(gradx_f)) if cpt==20: break We can display the solution onto the contour of $f$ or onto the contour of $L_c$\nevolution_X = evolution[:, 0] evolution_Y = evolution[:, 1] x1 = np.linspace(-2, 2, 25) y1 = np.linspace(-2, 2, 25) Xx, Yy = np.meshgrid(x1, y1) print(\u0026#34;mu\u0026#34;,mu) #mu=1.#0.5 #Z = Lc(Xx, Yy,mu=mu,c=0) Z = f(Xx, Yy) fig= plt.figure(figsize = (10,7)) contours = plt.contour(Xx, Yy, Z, 45) plt.clabel(contours, inline = True, fontsize = 10) plt.title(\u0026#34;Evolution of the cost function during gradient descent with level circles\u0026#34;, fontsize=15) plt.plot(evolution_X, evolution_Y) plt.plot(evolution_X, evolution_Y, \u0026#39;*\u0026#39;, label = \u0026#34;Cost function\u0026#34;) plt.xlabel(\u0026#39;x\u0026#39;, fontsize=11) plt.ylabel(\u0026#39;y\u0026#39;, fontsize=11) plt.colorbar() plt.legend(loc = \u0026#34;upper right\u0026#34;) plt.show() mu 0.5000001501871618 4. We add new constraints: $(x,y)$ must stay in $K=[-2.,0]\\times …","date":1727182759,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1727182759,"objectID":"ed2b200e4dca62049592fadd9d453f8b","permalink":"https://grosjean1.github.io/post/notebook4/","publishdate":"2024-09-24T14:59:19+02:00","relpermalink":"/post/notebook4/","section":"post","summary":"Gradient descent with projection and augmented lagrangian algorithm To find the infimum of an arbitrary cost function, we use here the gradient descent. Let us consider the function $f(x,y)=0.5(x^2+(y-1)^2)$ that we aim at minimizing on $K\\subset \\mathbb{R}^2$ with $K={(x,y) | x= y}$.","tags":[],"title":"Notebook4","type":"post"},{"authors":[],"categories":[],"content":"Gradient descent To find the infimum of an arbitrary cost function, we use here the gradient descent. Let us consider the function $f(x,y)=2x^2+y^2-xy+2$ that we aim at minimizing on $ \\mathbb{R}^2$. We start by manually compute its gradient and we will display its evolution during gradient descent.\nimport sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (2.0.0) \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -\u0026gt; \u001b[0m\u001b[32;49m24.2\u001b[0m \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/local/Cellar/jupyterlab/4.2.1/libexec/bin/python -m pip install --upgrade pip\u001b[0m Requirement already satisfied: matplotlib in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (3.9.1) Requirement already satisfied: contourpy\u0026gt;=1.0.1 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (1.2.1) Requirement already satisfied: cycler\u0026gt;=0.10 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (0.12.1) Requirement already satisfied: fonttools\u0026gt;=4.22.0 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (4.53.1) Requirement already satisfied: kiwisolver\u0026gt;=1.3.1 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (1.4.5) Requirement already satisfied: numpy\u0026gt;=1.23 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (2.0.0) Requirement already satisfied: packaging\u0026gt;=20.0 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (24.0) Requirement already satisfied: pillow\u0026gt;=8 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (10.4.0) Requirement already satisfied: pyparsing\u0026gt;=2.3.1 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (3.1.2) Requirement already satisfied: python-dateutil\u0026gt;=2.7 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0) Requirement already satisfied: six\u0026gt;=1.5 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from python-dateutil\u0026gt;=2.7-\u0026gt;matplotlib) (1.16.0) \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -\u0026gt; \u001b[0m\u001b[32;49m24.2\u001b[0m \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/local/Cellar/jupyterlab/4.2.1/libexec/bin/python -m pip install --upgrade pip\u001b[0m import numpy as np import matplotlib.pyplot as plt def f(x,y): return ... def df(x,y): return ... We need to define a norm to now how far from the global solution we are.\ndef norm(a): return np.sqrt(a[0]**2+a[1]**2) ## start GD algorithm eps=1e-6 alpha=0.1 #test with alpha=1 x0,y0=1,2 evolution = [[x0, y0]] grad_f = df(x0, y0) while ...\u0026gt;=eps: d = ... x0, y0 = ... evolution = ... grad_f = ... evolution_X = evolution[:, 0] evolution_Y = evolution[:, 1] x1 = np.linspace(-2, 2, 150) y1 = np.linspace(-2, 2, 150) Xx, Yy = np.meshgrid(x1, y1) Z = fxy(Xx, Yy) fig = plt.figure(figsize = (10,7)) contours = plt.contour(Xx, Yy, Z, 20) plt.clabel(contours, inline = True, fontsize = 10) plt.title(\u0026#34;Evolution of the cost function during gradient descent with level circles\u0026#34;, fontsize=15) plt.plot(evolution_X, evolution_Y) plt.plot(evolution_X, evolution_Y, \u0026#39;*\u0026#39;, label = \u0026#34;Cost function\u0026#34;) plt.xlabel(\u0026#39;x\u0026#39;, fontsize=11) plt.ylabel(\u0026#39;y\u0026#39;, fontsize=11) plt.colorbar() plt.legend(loc = \u0026#34;upper right\u0026#34;) plt.show() With $\\alpha=1$, the solution blows up, a correct choice for $\\alpha$ is very important. That is why, we will proceed with Armijo rule:\nc1=.6 beta=1. #test with beta=1 gamma=0.01 def phik(xyk,alphak,dk): return ... def dphik(xyk,alphak,dk): grad_f=... return ... def armijo_rule(alpha,xy_old,d): #d_x est la direction de descente d_x . grad_x \u0026lt;= 0 # test f(x_new) \\leq f(x_0) + c alpha ps{d_x}{grad_x} test = 1 iter=0 while test!=0 and iter\u0026lt;500: if phik(xy_old,alpha,d)\u0026lt;=...: test = 0 else: alpha*=gamma iter+=1 return alpha #initialization for Armijo alpha=beta ## start GD algorithm eps=1e-6 x0,y0=1,2 evolution = [[x0, y0]] grad_f = dfxy(x0, y0) cpt_grad=0 while norm(grad_f)\u0026gt;=eps and cpt_grad\u0026lt;500: d = -grad_f #armijo alpha=beta xy_old=np.array([x0,y0]) cpt=0 alpha=armijo_rule(alpha,xy_old,d) #while phik(xy_old,alpha,d)\u0026gt;phik(xy_old,0,d)+c1*dphik(xy_old,0,d)*alpha and cpt\u0026lt;500: # alpha*=gamma # cpt+=1 x0, y0 = x0 + alpha*d[0], y0 + alpha*d[1] evolution = np.vstack((evolution, [x0, y0])) grad_f = dfxy(x0, y0) cpt_grad+=1 evolution_X = evolution[:, 0] evolution_Y = evolution[:, 1] x1 = np.linspace(-2, 2, 150) y1 = np.linspace(-2, 2, 150) Xx, Yy = np.meshgrid(x1, y1) Z = fxy(Xx, Yy) fig = plt.figure(figsize = (10,7)) contours = …","date":1725566754,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725566754,"objectID":"5b4a949481c97ac3df94542bb48feedf","permalink":"https://grosjean1.github.io/post/notebook2/","publishdate":"2024-09-05T22:05:54+02:00","relpermalink":"/post/notebook2/","section":"post","summary":"Gradient descent To find the infimum of an arbitrary cost function, we use here the gradient descent. Let us consider the function $f(x,y)=2x^2+y^2-xy+2$ that we aim at minimizing on $ \\mathbb{R}^2$.","tags":[],"title":"Gradient descent","type":"post"},{"authors":[],"categories":[],"content":"#Install package import sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib !{sys.executable} -m pip install amplpy from amplpy import AMPL # import pyAMPL from amplpy import ampl_notebook ampl = ampl_notebook( modules=[\u0026#34;cplex\u0026#34;], # modules to install license_uuid=\u0026#34;default\u0026#34;, # license to use ) # instantiate AMPL object and register magics %%ampl_eval # define decision variables reset; # Declaration of optimization variables var xx; var yy; # Declaration of parameters param aa=-4; param bb=2; %%ampl_eval # Cost function minimize f: xx**2 + aa*(xx+yy) + 2*yy**2; # Constraints subject to g: xx+yy = bb; subject to h: xx \u0026gt;= 0; %%ampl_eval let xx:= 1; let yy:=2; # exhibit the model that has been built ampl.eval(\u0026#34;show;\u0026#34;) ampl.eval(\u0026#34;expand;\u0026#34;) # solve using two different solvers ampl.option[\u0026#34;solver\u0026#34;] = \u0026#34;cplex\u0026#34; ampl.solve() #ampl.option[\u0026#34;solver\u0026#34;] = \u0026#34;highs\u0026#34; #ampl.solve() ampl.display(\u0026#34;xx\u0026#34;);# xx,yy; ampl.display(\u0026#34;f\u0026#34;); ampl.display(\u0026#34;g.dual\u0026#34;); ampl.display(\u0026#34;h.dual\u0026#34;); ","date":1725566563,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725566563,"objectID":"3b6a8e6c6ea357c8a388cc5fa590c6ec","permalink":"https://grosjean1.github.io/post/notebook1/","publishdate":"2024-09-05T22:02:43+02:00","relpermalink":"/post/notebook1/","section":"post","summary":"#Install package import sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib !{sys.executable} -m pip install amplpy from amplpy import AMPL # import pyAMPL from amplpy import ampl_notebook ampl = ampl_notebook( modules=[\"cplex\"], # modules to install license_uuid=\"default\", # license to use ) # instantiate AMPL object and register magics %%ampl_eval # define decision variables reset; # Declaration of optimization variables var xx; var yy; # Declaration of parameters param aa=-4; param bb=2; %%ampl_eval # Cost function minimize f: xx**2 + aa*(xx+yy) + 2*yy**2; # Constraints subject to g: xx+yy = bb; subject to h: xx \u003e= 0; %%ampl_eval let xx:= 1; let yy:=2; # exhibit the model that has been built ampl.","tags":[],"title":"Notebook Pyampl tuto","type":"post"},{"authors":["Elise Grosjean","Alex Keilmann","Henry Jäger","Steffen Plunder","Claudia Redenbach","Bernd Simeon","Christina Surulescu"],"categories":[],"content":"","date":1717977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717977600,"objectID":"69c016b88deac2ee33b171f94a4c801b","permalink":"https://grosjean1.github.io/publication/publi7/","publishdate":"2024-06-10T00:00:00Z","relpermalink":"/publication/publi7/","section":"publication","summary":"We study the dynamics of a seeding experiment where a fibrous scaffold material is colonized by two types of cell populations. The specific application that we have in mind is related to the idea of meniscus tissue regeneration. In order to support the development of a promising replacement material, we discuss certain rate equations for the densities of human mesenchymal stem cells and chondrocytes and for the production of collagen-containing extracellular matrix. For qualitative studies, we start with a system of ordinary differential equations and refine then the model to include spatial effects of the underlying nonwoven scaffold structure. Numerical experiments as well as a complete set of parameters for future benchmarking are provided.","tags":[],"title":"Cell seeding dynamics in a porous scaffold material designed for meniscus tissue regeneration","type":"publication"},{"authors":["Elise Grosjean"],"categories":null,"content":"","date":1713744000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713744000,"objectID":"0b48aab82d484c3c9e3fb959be433ecf","permalink":"https://grosjean1.github.io/talk/seminaire-idefix-ensta-2024/","publishdate":"2024-04-22T00:00:00Z","relpermalink":"/talk/seminaire-idefix-ensta-2024/","section":"event","summary":"Présentation générale de mes travaux","tags":[],"title":"Séminaire Idefix ENSTA 2024","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":"","date":1710374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710374400,"objectID":"e983e000e8f114f09116cebdf69bf34b","permalink":"https://grosjean1.github.io/talk/seminaire-anedp-au-laboratoire-paul-painleve/","publishdate":"2024-03-14T00:00:00Z","relpermalink":"/talk/seminaire-anedp-au-laboratoire-paul-painleve/","section":"event","summary":"Analyse de sensibilité d'un modèle de régénération de tissu cellulaire et méthodes de base réduite non-intrusives deux-grilles","tags":[],"title":"Séminaire ANEDP au laboratoire Paul Painlevé","type":"event"},{"authors":["Elise Grosjean","Alex Keilmann","Henry Jäger","Shimi Mohanan","Claudia Redenbach","Bernd Simeon","Christina Surulescu","Luisa de Roy","Andreas Seitz","Graciosa Teixeira","Martin Dauner","Carsten Linti","Günter Schmidt"],"categories":[],"content":"","date":1709942400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709942400,"objectID":"d5cae5d870f64a3f441ae7d2513b1a12","permalink":"https://grosjean1.github.io/publication/publi6/","publishdate":"2024-03-09T00:00:00Z","relpermalink":"/publication/publi6/","section":"publication","summary":"We develop a model the dynamics of human mesenchymal stem cells (hMSCs) and chondrocytes evolving in a nonwoven polyethylene terephtalate (PET) scaffold impregnated with hyaluron and supplied with a differentiation medium. The scaffold and the cells are assumed to be contained in a bioreactor with fluid perfusion. The differentiation of hMSCs into chondrocytes favors the production of extracellular matrix (ECM) and is influenced by fluid stress. The model takes deformations of ECM and PET scaffold into account. The scaffold structure is explicitly included by statistical assessment of the fibre distribution from CT images. The effective macroscopic equations are obtained by appropriate upscaling from dynamics on lower (microscopic and mesoscopic) scales and feature in the motility terms an explicit cell diffusion tensor encoding the assessed anisotropic scaffold structure. Numerical simulations show its influence on the overall cell and tissue dynamics.","tags":[],"title":"An in-silico approach to meniscus tissue regeneration: Modeling, numerical simulation, and experimental analysis","type":"publication"},{"authors":["Elise Grosjean"],"categories":null,"content":"","date":1706486400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706486400,"objectID":"7d46a75cf09128e21c564baeffd77957","permalink":"https://grosjean1.github.io/talk/seminaire-lamfa/","publishdate":"2024-01-29T00:00:00Z","relpermalink":"/talk/seminaire-lamfa/","section":"event","summary":"Méthodes de base réduite non intrusives appliquée à l'analyse de sensibilité","tags":[],"title":"Séminaire Lamfa","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":"","date":1706140800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706140800,"objectID":"30806b3de7976043df768a898f6bebb1","permalink":"https://grosjean1.github.io/talk/seminaire-interne-m3disim-inria/","publishdate":"2024-01-25T00:00:00Z","relpermalink":"/talk/seminaire-interne-m3disim-inria/","section":"event","summary":"Sensitivity analysis with a RB approach","tags":[],"title":"Séminaire interne M3DISIM - INRIA","type":"event"},{"authors":["Elise Grosjean","Bernd Simeon"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"4e5417378d1c8634c4cc9f95768153c7","permalink":"https://grosjean1.github.io/publication/publi4/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/publi4/","section":"publication","summary":"This paper deals with the derivation of Non-Intrusive Reduced Basis (NIRB) techniques for sensitivity analysis, more specifically the direct and adjoint state methods. For highly complex parametric problems, these two approaches may become too costly. To reduce computational times, Proper Orthogonal Decomposition (POD) and Reduced Basis Methods (RBMs) have already been investigated. The majority of these algorithms are however intrusive in the sense that the High-Fidelity (HF) code must be modified. To address this issue, non-intrusive strategies are employed. The NIRB two-grid method uses the HF code solely as a ``black-box'', requiring no code modification. Like other RBMs, it is based on an offline-online decomposition. The offline stage is time-consuming, but it is only executed once, whereas the online stage is significantly less expensive than an HF evaluation. In this paper, we propose new NIRB two-grid algorithms for both the direct and adjoint state methods. On a classical model problem, the heat equation, we prove that HF evaluations of sensitivities reach an optimal convergence rate in L∞(0,T;H1(Ω)), and then establish that these rates are recovered by the proposed NIRB approximations. These results are supported by numerical simulations. We then numerically demonstrate that a further deterministic post-treatment can be applied to the direct method. This further reduces computational costs of the online step while only computing a coarse solution of the initial problem. All numerical results are run with the model problem as well as a more complex problem, namely the Brusselator system.","tags":[],"title":"Error estimate of the non-intrusive reduced basis two-grid method applied to sensitivity analysis (M2AN)","type":"publication"},{"authors":["Elise Grosjean","Yvon Maday"],"categories":[],"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701388800,"objectID":"1b5cc9e4fcdcba6d5677c2203abed7e4","permalink":"https://grosjean1.github.io/publication/publi3/","publishdate":"2023-12-01T00:00:00Z","relpermalink":"/publication/publi3/","section":"publication","summary":"Reduced Basis Methods (RBMs) are frequently proposed to approximate parametric problem solutions. They can be used to calculate solutions for a large number of parameter values (e.g. for parameter fitting) as well as to approximate a solution for a new parameter value (e.g. real time approximation with a very high accuracy). They intend to reduce the computational costs of High Fidelity (HF) codes. We will focus on the Non-Intrusive Reduced Basis (NIRB) two-grid method. Its main advantage is that it uses the HF code exclusively as a \"black-box,\" as opposed to other so-called intrusive methods that require code modification. This is very convenient when the HF code is a commercial one that has been purchased, as is frequently the case in the industry. The effectiveness of this method relies on its decomposition into two stages, one offline (classical in most RBMs as presented above) and one online. The offline part is time-consuming but it is only performed once. On the contrary, the specificity of this NIRB approach is that, during the online part, it solves the parametric problem on a coarse mesh only and then improves its precision. As a result, it is significantly less expensive than a HF evaluation. This method has been originally developed for elliptic equations with finite elements and has since been extended to finite volume. In this paper, we extend the NIRB two-grid method to parabolic equations. We recover optimal estimates in L∞(0,T;H1(Ω)) using as a model problem, the heat equation. Then, we present numerical results on the heat equation and on the Brusselator problem.","tags":[],"title":"Error estimate of the Non-Intrusive Reduced Basis (NIRB) two-grid method with parabolic equations, SMAI: JCM 9 (2023) 227-256","type":"publication"},{"authors":["Elise Grosjean"],"categories":null,"content":"","date":1695168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695168000,"objectID":"2dc838b15c2eced37e8a4a899f7dfb2f","permalink":"https://grosjean1.github.io/talk/iccb2023/","publishdate":"2023-09-20T00:00:00Z","relpermalink":"/talk/iccb2023/","section":"event","summary":"Meniscus tissue regeneration and sensitivity RB approach","tags":[],"title":"ICCB2023","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":" Presentation on the numerical aspects of a Coupled analysis of active biological processes for meniscus tissue regeneration Program ","date":1694476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694476800,"objectID":"0077ad4633c0f96be434e89651bfb232","permalink":"https://grosjean1.github.io/talk/spp2311-kick-off/","publishdate":"2023-09-12T00:00:00Z","relpermalink":"/talk/spp2311-kick-off/","section":"event","summary":"Coupled analysis of active biological processes for meniscus tissue regeneration","tags":[],"title":"SPP2311-Kick-off","type":"event"},{"authors":["Elise Grosjean","Bernd Simeon","Christina Surulescu"],"categories":[],"content":"","date":1692576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692576000,"objectID":"cf9d6b2158984e041997d7bb62d1e155","permalink":"https://grosjean1.github.io/publication/publi5/","publishdate":"2023-08-21T00:00:00Z","relpermalink":"/publication/publi5/","section":"publication","summary":"We propose a continuous model for meniscus cartilage regeneration triggered by two populations of cells migrating and (de)differentiating within an artificial scaffold with a known structure. The described biological processes are influenced by a fluid flow and therewith induced deformations of the scaffold. Numerical simulations are done for the corresponding dynamics within a bioreactor which was designed for performing the biological experiments.","tags":[],"title":"A mathematical model for meniscus cartilage regeneration (PAMM)","type":"publication"},{"authors":["Elise Grosjean"],"categories":null,"content":"","date":1685404800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685404800,"objectID":"9a09db21391e1d3a5fab65b2f69e3b53","permalink":"https://grosjean1.github.io/talk/gamm-annual-meeting-2023/","publishdate":"2023-05-30T00:00:00Z","relpermalink":"/talk/gamm-annual-meeting-2023/","section":"event","summary":"GAMM 2023","tags":[],"title":"GAMM Annual Meeting 2023","type":"event"},{"authors":["Elise Grosjean","Camille Pouchol"],"categories":null,"content":"","date":1672790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672790400,"objectID":"b0bd9fd8d6514f9ff7a8cb0a8b038c45","permalink":"https://grosjean1.github.io/talk/campus-france-presentation/","publishdate":"2023-01-04T00:00:00Z","relpermalink":"/talk/campus-france-presentation/","section":"event","summary":"Studying mathematics in France","tags":[],"title":"Campus France Presentation","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":" Abstract ","date":1669334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669334400,"objectID":"caf031fac111a02cf173c1ed70da7a66","permalink":"https://grosjean1.github.io/talk/map5-groupe-de-travail-modelisation-analyse-simulation/","publishdate":"2022-11-25T00:00:00Z","relpermalink":"/talk/map5-groupe-de-travail-modelisation-analyse-simulation/","section":"event","summary":"Meniscus tissue regeneration - sensibility analysis - non-intrusive reduced basis method","tags":[],"title":"MAP5 Groupe de travail Modelisation, Analyse, Simulation","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":"Differential-Algebraic Equations Website\n","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"7540a43924c6476503264dee5f496f37","permalink":"https://grosjean1.github.io/teaching/tuk/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/teaching/tuk/","section":"teaching","summary":"Differential-Algebraic Equations Website","tags":[],"title":"Tutor at Department of Mathematics, TU Kaiserslautern","type":"teaching"},{"authors":["Elise Grosjean"],"categories":null,"content":" Talk on non-intrusive reduced basis methods applied to parabolic equations. ","date":1655078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655078400,"objectID":"d3db4069812bf15464e91203772e23e8","permalink":"https://grosjean1.github.io/talk/canum-2022/","publishdate":"2022-06-13T00:00:00Z","relpermalink":"/talk/canum-2022/","section":"event","summary":" Talk on non-intrusive reduced basis methods applied to parabolic equations. ","tags":[],"title":"CANUM 2022","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":" Presentation on non intrusive reduced basis 2-grid method applied to wind farm simulations. Program ","date":1653868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653868800,"objectID":"e64723ea1a0bb8aa7dd4ca3eb08bc006","permalink":"https://grosjean1.github.io/talk/simulation-and-optimization-for-renewable-marine-energies/","publishdate":"2022-05-30T00:00:00Z","relpermalink":"/talk/simulation-and-optimization-for-renewable-marine-energies/","section":"event","summary":"Talk on Reduced basis method applied to wind farms","tags":[],"title":"Simulation and Optimization for Renewable Marine Energies","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":" Presentation on the numerical aspects of a Coupled analysis of active biological processes for meniscus tissue regeneration Program ","date":1653264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653264000,"objectID":"eb7a9a86204b8a46306b95b46b55d9bf","permalink":"https://grosjean1.github.io/talk/spp2311-kick-off/","publishdate":"2022-05-23T00:00:00Z","relpermalink":"/talk/spp2311-kick-off/","section":"event","summary":"Coupled analysis of active biological processes for meniscus tissue regeneration","tags":[],"title":"SPP2311-Kick-off","type":"event"},{"authors":["Elise Grosjean","Yvon Maday"],"categories":[],"content":"","date":1643673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643673600,"objectID":"9c373b2a3ef7cbfeb018c366ca19b9fd","permalink":"https://grosjean1.github.io/publication/publi2/","publishdate":"2022-02-01T00:00:00Z","relpermalink":"/publication/publi2/","section":"publication","summary":"Reduced Basis Methods (RBM) are often proposed to approximate the solutions of a parameter-dependent problem for a large number of parameter values, as an alternative to classical solvers, in order to reduce the computational costs. They usually are decomposed in two stages. One stage is done offline and can be considered as a learning procedure where a Reduced Basis (RB) is built from several solutions, called snapshots, computed with a high fidelity (HF) classical method, involving, e.g. a fine mesh (finite element or finite volume). In the second stage, which is online and has to be very cheap, a reduced basis problem is solved. The efficiency of the RBM relies on the ability, offline, to prepare the online step. In this article, we consider a Non-Intrusive RBM (NIRB) which is the two grids method. One fine mesh is used to construct the snapshots for the generation of the RB. Then, in the online part, the NIRB algorithm involves a coarse mesh where the problem solution for a new parameter is approximated and is $L^2$-projected on the RB. We adapt the NIRB algorithm to improve the accuracy and to further reduce the complexity of the online part, in particular by using a \\emph{domain truncation}. We exploit the fact that the solutions of parameterized problems behave physically similarly for a suitable range of parameters. We create two RB and a deterministic (algebraic) process which allows us to pass from one to the other. This new algorithm is applied to the 2D backward facing step. The second aim of this article is to deal with \\emph{singularities}. The domain geometry produces a fluid recirculation zone that must be captured correctly, for instance with a refinement of the mesh. The two-grid method in the FEM context is applied with a new stategy in order to counterbalance the effects of the singularities. The channel domain considered in the model problem has one re-entrant corner and thus the convergence is not optimal with uniform meshes. Thus, the theory of the two-grid method does not applied. However, we present several numerical results with fine refined meshes where both NIRB approaches succeed in retrieving the fine FEM accuracy.","tags":[],"title":"A doubly reduced approximation for the solution to PDEs based on a domain truncation and a reduced basis method: Application to Navier-Stokes equations (preprint)","type":"publication"},{"authors":["Elise Grosjean"],"categories":null,"content":" Presentation on optimizations of a non-intrusive reduced basis method Program ","date":1632182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632182400,"objectID":"5f725f33c23522440157b626409b9b9c","permalink":"https://grosjean1.github.io/talk/workshop-mathematics-of-high-performance-computing/","publishdate":"2021-09-21T00:00:00Z","relpermalink":"/talk/workshop-mathematics-of-high-performance-computing/","section":"event","summary":"Talk on optimizations of a non-intrusive reduced basis method","tags":[],"title":"Workshop Mathematics of High-Performance Computing","type":"event"},{"authors":["Elise Grosjean","Yvon Maday"],"categories":[],"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"a547b42cd6db23fe12bbe4f591488f4d","permalink":"https://grosjean1.github.io/publication/publi1/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/publication/publi1/","section":"publication","summary":"The context of this paper is the simulation of parameter-dependent partial differential equations (PDEs). When the aim is to solve such PDEs for a large number of parameter values, Reduced Basis Methods (RBM) are often used to reduce computational costs of a classical high fidelity code based on Finite Element Method (FEM), Finite Volume (FVM) or Spectral methods. The efficient implementation of most of these RBM requires to modify this high fidelity code, which cannot be done, for example in an industrial context if the high fidelity code is only accessible as a \"black-box\" solver. The Non-Intrusive Reduced Basis (NIRB) method has been introduced in the context of finite elements as a good alternative to reduce the implementation costs of these parameter-dependent problems. The method is efficient in other contexts than the FEM one, like with finite volume schemes, which are more often used in an industrial environment. In this case, some adaptations need to be done as the degrees of freedom in FV methods have different meanings. At this time, error estimates have only been studied with FEM solvers. In this paper, we present a generalisation of the NIRB method to Finite Volume schemes and we show that estimates established for FEM solvers also hold in the FVM setting. We first prove our results for the hybrid-Mimetic Finite Difference method (hMFD), which is part the Hybrid Mixed Mimetic methods (HMM) family. Then, we explain how these results apply more generally to other FV schemes. Some of them are specified, such as the Two Point Flux Approximation (TPFA).","tags":[],"title":"Error estimate of the Non-Intrusive Reduced Basis method with finite volume schemes, ESAIM: M2AN 55 (2021) 1941–1961","type":"publication"},{"authors":["Elise Grosjean"],"categories":null,"content":" Talk on non intrusive reduced basis methods applied to finite volume schemes. ","date":1606953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606953600,"objectID":"e8b1a738c1d699a9d5736e492fcceb27","permalink":"https://grosjean1.github.io/talk/can-j-2020/","publishdate":"2020-12-03T00:00:00Z","relpermalink":"/talk/can-j-2020/","section":"event","summary":" Talk on non intrusive reduced basis methods applied to finite volume schemes. ","tags":[],"title":"CAN-J 2020","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":" Presentation on non intrusive reduced basis methods applied to finite volume schemes. ","date":1602028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602028800,"objectID":"2789c7636ee8a9431a1e5184fa1c12e6","permalink":"https://grosjean1.github.io/talk/gtt-ljll/","publishdate":"2020-10-07T00:00:00Z","relpermalink":"/talk/gtt-ljll/","section":"event","summary":"Presentation on Reduced basis methods","tags":[],"title":"GTT LJLL","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":" Presentation on non intrusive reduced basis methods applied to finite volume schemes. Program ","date":1599609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599609600,"objectID":"3680febafcd08d785b512942f800e5d0","permalink":"https://grosjean1.github.io/talk/model-order-reduction-summer-school-morss2020/","publishdate":"2020-09-09T00:00:00Z","relpermalink":"/talk/model-order-reduction-summer-school-morss2020/","section":"event","summary":"Talk on Reduced basis method applied to wind farms","tags":[],"title":"Model Order Reduction Summer School MORSS2020","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":" Link to the project dataset: smart_home_device_usage_data.csv\nLink to the project notebook: notebook\nlink to slides:\nslides 1\nslides 2\nslides 3\nslides 4\nslides 5\nscenarios.mat for TP2\nOther test\nlink to TP 1: Dynamic programming link to TP 2: Dynamic programming and autoregressive processes link to TP 3: Dynamic programming and autoregressive processes link to TP 4: Supervised classification link to TP 5: Supervised classification ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"7671315caf67f5f9aaeaaecb96f1d88b","permalink":"https://grosjean1.github.io/teaching/ensta_apm_5en06_ta/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/teaching/ensta_apm_5en06_ta/","section":"teaching","summary":"Link to the project dataset: smart_home_device_usage_data.csv\nLink to the project notebook: notebook\nlink to slides:\nslides 1\nslides 2\nslides 3\nslides 4\nslides 5\nscenarios.mat for TP2\nOther test","tags":[],"title":"APM_5EN06_TA at ENSTA-Paris","type":"teaching"},{"authors":["Elise Grosjean"],"categories":null,"content":"link to ENSTA course description\nlink to tutorial : ampl and in python pyampl\nlink to first TP exercice: Gradient Descent TP\nlink to second TP exercice: [Gradient Descent with Augmented Lagrangian]\nlink to second TP exercice:\nlink to slides - part I : slides - part 1\nlink to slides - part II : slides - part 2\nlink to slides - part III : [slides - part 3]\nlink to TP - part I: TP 1 The goal of this first TP is to know how to launch jupyter notebook, save a notebook, launch amplpy package, model an optimization problem and solve it\nlink to TP - part I: [TP 2]\nUn corrigé d’exercice: [Corrige]\nSlides exercices en cours [ici]\nlink to exercice 6 : [exo 6]\nlink to exercice 7 : [exo 7]\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"486439fdd7e20059b52b3bb0dcd6b885","permalink":"https://grosjean1.github.io/teaching/ensta/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/teaching/ensta/","section":"teaching","summary":"link to ENSTA course description\nlink to tutorial : ampl and in python pyampl\nlink to first TP exercice: Gradient Descent TP\nlink to second TP exercice: [Gradient Descent with Augmented Lagrangian]","tags":[],"title":"APM_5EN5A_TA at ENSTA-Paris","type":"teaching"},{"authors":["Elise Grosjean"],"categories":null,"content":"link to ENSAE website\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"14207e61e9f0b1c43a0b4fc657a59504","permalink":"https://grosjean1.github.io/teaching/ensae/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/teaching/ensae/","section":"teaching","summary":"link to ENSAE website","tags":[],"title":"Tutor at l'école nationale de la statistique et de l'administration économique (ENSAE)","type":"teaching"},{"authors":["Elise Grosjean"],"categories":null,"content":" Poster presentation on non intrusive reduced basis methods applied to wind farms. Painting exhibition (see above) ","date":1574812800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574812800,"objectID":"97adff086625845d3bcc9d91c8cb7079","permalink":"https://grosjean1.github.io/talk/poster-session-50-ljll/","publishdate":"2019-11-27T00:00:00Z","relpermalink":"/talk/poster-session-50-ljll/","section":"event","summary":"Poster presentation on Reduced basis method applied to wind farms","tags":[],"title":"Poster Session 50 LJLL","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":" 4M026 Approximation des équations aux derivées partielles\nAnalyse numérique matricielle, TD/TP L3.\nAnalyse numérique pour les EDO, TP L3.\nProgrammation Python\n","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"549ae6a66febaa1770b1caf5cc874db2","permalink":"https://grosjean1.github.io/teaching/sorbonne/","publishdate":"2018-09-01T00:00:00Z","relpermalink":"/teaching/sorbonne/","section":"teaching","summary":"TDs et TPs (monitorat)","tags":[],"title":"Tutor at Sorbonne université","type":"teaching"},{"authors":null,"categories":null,"content":"Gradient descent with projection and augmented lagrangian algorithm To find the infimum of an arbitrary cost function, we use here the gradient descent. Let us consider the function $f(x,y)=0.5(x^2+(y-1)^2)$ that we aim at minimizing on $K\\subset \\mathbb{R}^2$ with $K={(x,y) | x= y}$. We start by manually compute its gradient and we will display its evolution during gradient descent.\nimport sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib 1. Complete the following code import numpy as np import matplotlib.pyplot as plt def f(x1,x2): # function return ... def df(x1,x2): # gradient return ... def g(x1,x2): # constraint return ... def dg(x1,x2): # constraint gradient return ... def L(x1,x2,mu): # lagrangian return ... def Lc(x1,x2,mu,c): # augmented lagrangian return ... def dxLc(x1,x2,mu,c): # augmented lagrangian gradient return ... We need to define a norm to now how far from the global solution we are.\ndef norm(a): return np.sqrt(a[0]**2+a[1]**2) With $\\alpha$ too big, we found out that the solution blows up in the previous exercice, a correct choice for $\\alpha$ is very important. That is why, we will proceed with Armijo-Wolfe rule:\n# Wolfe rule: c1=.6 c2=.8 beta=1. #test with beta=1 eta=2. gamma=0.01 def phik(xyk,alphak,dk,fc,mu,c): return fc(xyk[0]+alphak*dk[0],xyk[1]+alphak*dk[1],mu,c) def dphik(xyk,alphak,dk,dfc,mu,c): grad_f=dfc(xyk[0]+alphak*dk[0],xyk[1]+alphak*dk[1],mu,c) return np.dot(grad_f,dk) def wolfe_rule(alpha,xy_old,d,fc,dfc,mu=0,c=0): #d_x est la direction de descente d_x . grad_x \u0026lt;= 0 # test f(x_new) \\leq f(x_0) + c alpha ps{d_x}{grad_x} test = 1 iteration = 0 min_ = 0 max_ = 1000 while (phik(xy_old,alpha,d,fc,mu,c)\u0026lt;=(phik(xy_old,0,d,fc,mu,c)+c1*dphik(xy_old,0,d,dfc,mu,c)*alpha)) \u0026amp;(iteration\u0026lt;=500): #armijo ok alpha=eta*alpha iteration = 0 while (test!=0)\u0026amp;(iteration\u0026lt;=500): xnew0,xnew1=xy_old[0]+alpha*d[0],xy_old[1]+alpha*d[1] if (phik(xy_old,alpha,d,fc,mu,c)\u0026lt;= (phik(xy_old,0,d,fc,mu,c)+c1*dphik(xy_old,0,d,dfc,mu,c)*alpha)) \u0026amp; (np.dot(dfc(xnew0,xnew1,mu,c),d) \u0026gt;= c2*np.dot(dfc(xy_old[0],xy_old[1],mu,c),d) ): test = 0 elif phik(xy_old,alpha,d,fc,mu,c)\u0026gt; (phik(xy_old,0,d,fc,mu,c)+c1*dphik(xy_old,0,d,dfc,mu,c)*alpha): #no armijo max_ = alpha alpha = (max_ + min_)/2 iteration = iteration +1 else: # armijo ok minorant = alpha alpha = (max_ + min_)/2 iteration = iteration +1 return alpha 2. Now we proceed with the Gradient descent algorithm with Armijo-Wolfe rule: Complete the following code def GradientDescent(fc,dfc,xy0,beta,mu=0,c=0): #initialization for Armijo alpha=beta # start GD algorithm eps=1e-6 x0,y0=xy0[0],xy0[1] #starting points evolution = [[x0, y0]] grad_f = ... #gradient function of Lc cpt_grad=0 while norm(grad_f)\u0026gt;=eps and cpt_grad\u0026lt;500: d = ... #direction alpha=beta #armijo xy_old=np.array([x0,y0]) alpha=wolfe_rule(alpha,xy_old,d,fc,dfc, mu=mu,c=c) # find best alpha x0, y0 = ... evolution = np.vstack((evolution, [x0, y0])) grad_f = ... cpt_grad+=1 return evolution 3. Complete the code of the augmented lagrangian: #initialization for Armijo alpha=beta ## start GD algorithm eps=1e-6 # initial points x1,x2=1.,1. c=1. # penalty parameter mu=1.7 # dual variable gold=... # not a golden function, but the constraints are very precious gradx_f = ... # gradient of the augmented lagrangian cpt=0 # to avoid infinite while loop xy0=[x1,x2] while norm(gradx_f)\u0026gt;eps or np.abs(g(x1,x2))\u0026gt;eps : evolution=... evolution_X = evolution[:, 0] evolution_Y = evolution[:, 1] x1,x2=evolution_X[-1],evolution_Y[-1] # last points print(\u0026#34;x1:\u0026#34;, x1, \u0026#34; x2:\u0026#34;, x2) if (np.abs(g(x1,x2)))\u0026lt;np.abs(gold): # if ||g|| is small #print(\u0026#34;||g|| smaller\u0026#34;,np.abs(g(x1,x2))) mu=... #increase mu print(\u0026#34;new mu: \u0026#34;, mu) else: c=... # increase c print(\u0026#34;new c: \u0026#34;,c) cpt+=1 gold=... #updates gradx_f = ... print(\u0026#34;norm of gradx_f: \u0026#34;,norm(gradx_f)) if cpt==20: break We can display the solution onto the contour of $f$ or onto the contour of $L_c$\nevolution_X = evolution[:, 0] evolution_Y = evolution[:, 1] x1 = np.linspace(-2, 2, 25) y1 = np.linspace(-2, 2, 25) Xx, Yy = np.meshgrid(x1, y1) print(\u0026#34;mu\u0026#34;,mu) #mu=1.#0.5 #Z = Lc(Xx, Yy,mu=mu,c=0) Z = f(Xx, Yy) fig= plt.figure(figsize = (10,7)) contours = plt.contour(Xx, Yy, Z, 45) plt.clabel(contours, inline = True, fontsize = 10) plt.title(\u0026#34;Evolution of the cost function during gradient descent with level circles\u0026#34;, fontsize=15) plt.plot(evolution_X, evolution_Y) plt.plot(evolution_X, evolution_Y, \u0026#39;*\u0026#39;, label = \u0026#34;Cost function\u0026#34;) plt.xlabel(\u0026#39;x\u0026#39;, fontsize=11) plt.ylabel(\u0026#39;y\u0026#39;, fontsize=11) plt.colorbar() plt.legend(loc = \u0026#34;upper right\u0026#34;) plt.show() mu 0.5000001501871618 4. We add new constraints: $(x,y)$ must stay in $K=[-2.,0]\\times [-2,0]$. Complete the code that finds the projection of $(x,y)$ on a cuboid $K$. Then, change the gradient descent algorithm to take into account this projection step def proj(K,x): #K=[[l0,u0],[l1,u1]] l0=... u0=... l1=... u1=... return ...,... # NEW GRADIENT DESCENT ALGORITHM: def GradientDescent(fc,dfc,xy0,beta,mu=0,c=0): #initialization for …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9110f9c5ebe81c74f6a55f3326d453aa","permalink":"https://grosjean1.github.io/post/notebook3/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/notebook3/","section":"post","summary":"Gradient descent with projection and augmented lagrangian algorithm To find the infimum of an arbitrary cost function, we use here the gradient descent. Let us consider the function $f(x,y)=0.5(x^2+(y-1)^2)$ that we aim at minimizing on $K\\subset \\mathbb{R}^2$ with $K={(x,y) | x= y}$.","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"26a5d48806f78bd36cb88674e88b8f5d","permalink":"https://grosjean1.github.io/miscellaneous/drawings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/miscellaneous/drawings/","section":"miscellaneous","summary":"","tags":null,"title":"My drawings/paintings (Slides)","type":"landing"}]