
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":["admin"],"categories":null,"content":"I am an Assistant Professor in applied mathematics at ENSTA Paris.\nMy research interests include fluid mechanics, P.D.E analysis, numerical schemes, reduced order methods, and inverse problems.\n","date":1717977600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1717977600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am an Assistant Professor in applied mathematics at ENSTA Paris.\nMy research interests include fluid mechanics, P.D.E analysis, numerical schemes, reduced order methods, and inverse problems.","tags":null,"title":"Elise Grosjean","type":"authors"},{"authors":["Elise Grosjean"],"categories":null,"content":"","date":1730419200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730419200,"objectID":"a8096832b17da148ec475f67cf31d27d","permalink":"https://example.com/talk/mathematics-and-art-exploring-connections/","publishdate":"2024-11-01T00:00:00Z","relpermalink":"/talk/mathematics-and-art-exploring-connections/","section":"event","summary":"","tags":[],"title":"Mathematics and Art, exploring connections","type":"event"},{"authors":[],"categories":[],"content":"data; #incoming flow param :trange: Q:= 1 386 2 346 3 416 4 713 5 1532 6 2000 7 1982 8 1431 9 780 10 476 11 450 12 420 ; # revenue param B:= 1 3500 2 700 3 2250 4 500 ; param d : 1 2 3 4 5 6 7 8 9 10 11 12:= 1 3 3 3 2 4 5 5 4 5 0 0 0 2 0 0 0 1 3 3 0 0 0.2 0 0 0 3 0 0 0 4.5 7 8 8 7 0.5 0 0 0 4 0 0 0 0.5 2 3.5 3.5 2.5 0.5 0 0 0 ; model; ","date":1728975550,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728975550,"objectID":"f65e801e2d246e6bca5410e80db74226","permalink":"https://example.com/post/exo7/","publishdate":"2024-10-15T08:59:10+02:00","relpermalink":"/post/exo7/","section":"post","summary":"data; #incoming flow param :trange: Q:= 1 386 2 346 3 416 4 713 5 1532 6 2000 7 1982 8 1431 9 780 10 476 11 450 12 420 ; # revenue param B:= 1 3500 2 700 3 2250 4 500 ; param d : 1 2 3 4 5 6 7 8 9 10 11 12:= 1 3 3 3 2 4 5 5 4 5 0 0 0 2 0 0 0 1 3 3 0 0 0.","tags":[],"title":"Exo7","type":"post"},{"authors":[],"categories":[],"content":"data; param:nrange :t:= 1 1790 2 1800 3 1810 4 1820 5 1830 6 1840 7 1850 8 1860 9 1870 10 1880 11 1890 12 1900 13 1910 14 1920 15 1930 16 1940 17 1950 18 1960 19 1970 20 1980 21 1990 22 2000; param P:= 1 3.93 2 5.31 3 7.24 4 9.64 5 12.86 6 17.06 7 23.19 8 31.44 9 38.56 10 50.19 11 62.98 12 76.21 13 92.23 14 106.02 15 123.20 16 132.16 17 151.33 18 179.32 19 203.30 20 226.54 21 248.71 22 281.42 ; model; ","date":1728974892,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728974892,"objectID":"76cd5b3ec59936308ca7dcca65384378","permalink":"https://example.com/post/exo6/","publishdate":"2024-10-15T08:48:12+02:00","relpermalink":"/post/exo6/","section":"post","summary":"data; param:nrange :t:= 1 1790 2 1800 3 1810 4 1820 5 1830 6 1840 7 1850 8 1860 9 1870 10 1880 11 1890 12 1900 13 1910 14 1920 15 1930 16 1940 17 1950 18 1960 19 1970 20 1980 21 1990 22 2000; param P:= 1 3.","tags":[],"title":"Exo6","type":"post"},{"authors":[],"categories":[],"content":"Gradient descent with projection and augmented lagrangian algorithm To find the infimum of an arbitrary cost function, we use here the gradient descent. Let us consider the function $f(x,y)=0.5(x^2+(y-1)^2)$ that we aim at minimizing on $K\\subset \\mathbb{R}^2$ with $K={(x,y) | x= y}$. We start by manually compute its gradient and we will display its evolution during gradient descent.\nimport sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib 1. Complete the following code import numpy as np import matplotlib.pyplot as plt def f(x1,x2): # function return 0.5*(x1**2+(x2-1)**2) def df(x1,x2): # gradient return np.array([x1,x2-1]) def g(x1,x2): # constraint return -x1+x2 def dg(x1,x2): # constraint gradient return np.array([-1,1]) def L(x1,x2,mu): # lagrangian return f(x1,x2)+mu* (g(x1,x2)) def Lc(x1,x2,mu,c): # augmented lagrangian return f(x1,x2)+c/2.* np.dot(g(x1,x2),g(x1,x2))+mu* (g(x1,x2)) def dxLc(x1,x2,mu,c): # augmented lagrangian gradient return df(x1,x2)+ c* np.dot(g(x1,x2),dg(x1,x2)) +mu* (dg(x1,x2)) #(x1-x2) (1,-1) We need to define a norm to now how far from the global solution we are.\ndef norm(a): return np.sqrt(a[0]**2+a[1]**2) With $\\alpha$ too big, we found out that the solution blows up in the previous exercice, a correct choice for $\\alpha$ is very important. That is why, we will proceed with Armijo-Wolfe rule:\n# Wolfe rule: c1=.6 c2=.8 beta=1. #test with beta=1 eta=2. gamma=0.01 def phik(xyk,alphak,dk,fc,mu,c): return fc(xyk[0]+alphak*dk[0],xyk[1]+alphak*dk[1],mu,c) def dphik(xyk,alphak,dk,dfc,mu,c): grad_f=dfc(xyk[0]+alphak*dk[0],xyk[1]+alphak*dk[1],mu,c) return np.dot(grad_f,dk) def wolfe_rule(alpha,xy_old,d,fc,dfc,mu=0,c=0): #d_x est la direction de descente d_x . grad_x \u0026lt;= 0 # test f(x_new) \\leq f(x_0) + c alpha ps{d_x}{grad_x} test = 1 iteration = 0 min_ = 0 max_ = 1000 while (phik(xy_old,alpha,d,fc,mu,c)\u0026lt;=(phik(xy_old,0,d,fc,mu,c)+c1*dphik(xy_old,0,d,dfc,mu,c)*alpha)) \u0026amp;(iteration\u0026lt;=500): #armijo ok alpha=eta*alpha iteration = 0 while (test!=0)\u0026amp;(iteration\u0026lt;=500): xnew0,xnew1=xy_old[0]+alpha*d[0],xy_old[1]+alpha*d[1] if (phik(xy_old,alpha,d,fc,mu,c)\u0026lt;= (phik(xy_old,0,d,fc,mu,c)+c1*dphik(xy_old,0,d,dfc,mu,c)*alpha)) \u0026amp; (np.dot(dfc(xnew0,xnew1,mu,c),d) \u0026gt;= c2*np.dot(dfc(xy_old[0],xy_old[1],mu,c),d) ): test = 0 elif phik(xy_old,alpha,d,fc,mu,c)\u0026gt; (phik(xy_old,0,d,fc,mu,c)+c1*dphik(xy_old,0,d,dfc,mu,c)*alpha): #no armijo max_ = alpha alpha = (max_ + min_)/2 iteration = iteration +1 else: # armijo ok minorant = alpha alpha = (max_ + min_)/2 iteration = iteration +1 return alpha 2. Now we proceed with the Gradient descent algorithm with Armijo-Wolfe rule: Complete the following code def GradientDescent(fc,dfc,xy0,beta,mu=0,c=0): #initialization for Armijo alpha=beta # start GD algorithm eps=1e-6 x0,y0=xy0[0],xy0[1] #starting points evolution = [[x0, y0]] grad_f = dfc(x0, y0,mu=mu,c=c) #gradient function cpt_grad=0 while norm(grad_f)\u0026gt;=eps and cpt_grad\u0026lt;500: d = -grad_f #direction alpha=beta #armijo xy_old=np.array([x0,y0]) alpha=wolfe_rule(alpha,xy_old,d,fc,dfc, mu=mu,c=c) # find best alpha x0, y0 = x0 + alpha*d[0], y0 + alpha*d[1] evolution = np.vstack((evolution, [x0, y0])) grad_f = dfc(x0, y0,mu=mu,c=c) cpt_grad+=1 return evolution 3. Complete the code of the augmented lagrangian: #initialization for Armijo alpha=beta ## start GD algorithm eps=1e-6 # initial points x1,x2=1.,1. c=1. # penalty parameter mu=1.7 # dual variable gold=g(x1,x2) # not a golden function, but the constraints are very precious gradx_f = dxLc(x1, x2,mu=mu,c=c) # gradient of the augmented lagrangian cpt=0 # to avoid infinite while loop xy0=[x1,x2] while norm(gradx_f)\u0026gt;eps or np.abs(g(x1,x2))\u0026gt;eps : evolution=GradientDescent(Lc,dxLc,xy0,beta,mu=mu,c=c) evolution_X = evolution[:, 0] evolution_Y = evolution[:, 1] x1,x2=evolution_X[-1],evolution_Y[-1] # last points print(\u0026#34;x1:\u0026#34;, x1, \u0026#34; x2:\u0026#34;, x2) if (np.abs(g(x1,x2)))\u0026lt;np.abs(gold): # if ||g|| is small #print(\u0026#34;||g|| smaller\u0026#34;,np.abs(g(x1,x2))) mu=mu+c*g(x1,x2) #increase mu print(\u0026#34;new mu: \u0026#34;, mu) else: c=c*2 # increase c print(\u0026#34;new c: \u0026#34;,c) cpt+=1 gold=g(x1,x2) #updates gradx_f = dxLc(x1, x2,mu,c) print(\u0026#34;norm of gradx_f: \u0026#34;,norm(gradx_f)) if cpt==20: break We can display the solution onto the contour of $f$ or onto the contour of $L_c$\nevolution_X = evolution[:, 0] evolution_Y = evolution[:, 1] x1 = np.linspace(-2, 2, 25) y1 = np.linspace(-2, 2, 25) Xx, Yy = np.meshgrid(x1, y1) print(\u0026#34;mu\u0026#34;,mu) #mu=1.#0.5 #Z = Lc(Xx, Yy,mu=mu,c=0) Z = f(Xx, Yy) fig= plt.figure(figsize = (10,7)) contours = plt.contour(Xx, Yy, Z, 45) plt.clabel(contours, inline = True, fontsize = 10) plt.title(\u0026#34;Evolution of the cost function during gradient descent with level circles\u0026#34;, fontsize=15) plt.plot(evolution_X, evolution_Y) plt.plot(evolution_X, evolution_Y, \u0026#39;*\u0026#39;, label = \u0026#34;Cost function\u0026#34;) plt.xlabel(\u0026#39;x\u0026#39;, fontsize=11) plt.ylabel(\u0026#39;y\u0026#39;, fontsize=11) plt.colorbar() plt.legend(loc = \u0026#34;upper right\u0026#34;) plt.show() mu 0.5000001501871618 4. We add new constraints: $(x,y)$ must stay in $K=[-2.,0]\\times …","date":1727182759,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1727182759,"objectID":"ed2b200e4dca62049592fadd9d453f8b","permalink":"https://example.com/post/notebook4/","publishdate":"2024-09-24T14:59:19+02:00","relpermalink":"/post/notebook4/","section":"post","summary":"Gradient descent with projection and augmented lagrangian algorithm To find the infimum of an arbitrary cost function, we use here the gradient descent. Let us consider the function $f(x,y)=0.5(x^2+(y-1)^2)$ that we aim at minimizing on $K\\subset \\mathbb{R}^2$ with $K={(x,y) | x= y}$.","tags":[],"title":"Notebook4","type":"post"},{"authors":[],"categories":[],"content":"Gradient descent To find the infimum of an arbitrary cost function, we use here the gradient descent. Let us consider the function $f(x,y)=2x^2+y^2-xy+2$ that we aim at minimizing on $ \\mathbb{R}^2$. We start by manually compute its gradient and we will display its evolution during gradient descent.\nimport sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (2.0.0) \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -\u0026gt; \u001b[0m\u001b[32;49m24.2\u001b[0m \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/local/Cellar/jupyterlab/4.2.1/libexec/bin/python -m pip install --upgrade pip\u001b[0m Requirement already satisfied: matplotlib in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (3.9.1) Requirement already satisfied: contourpy\u0026gt;=1.0.1 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (1.2.1) Requirement already satisfied: cycler\u0026gt;=0.10 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (0.12.1) Requirement already satisfied: fonttools\u0026gt;=4.22.0 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (4.53.1) Requirement already satisfied: kiwisolver\u0026gt;=1.3.1 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (1.4.5) Requirement already satisfied: numpy\u0026gt;=1.23 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (2.0.0) Requirement already satisfied: packaging\u0026gt;=20.0 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (24.0) Requirement already satisfied: pillow\u0026gt;=8 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (10.4.0) Requirement already satisfied: pyparsing\u0026gt;=2.3.1 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (3.1.2) Requirement already satisfied: python-dateutil\u0026gt;=2.7 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0) Requirement already satisfied: six\u0026gt;=1.5 in /usr/local/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from python-dateutil\u0026gt;=2.7-\u0026gt;matplotlib) (1.16.0) \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -\u0026gt; \u001b[0m\u001b[32;49m24.2\u001b[0m \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/local/Cellar/jupyterlab/4.2.1/libexec/bin/python -m pip install --upgrade pip\u001b[0m import numpy as np import matplotlib.pyplot as plt def f(x,y): return ... def df(x,y): return ... We need to define a norm to now how far from the global solution we are.\ndef norm(a): return np.sqrt(a[0]**2+a[1]**2) ## start GD algorithm eps=1e-6 alpha=0.1 #test with alpha=1 x0,y0=1,2 evolution = [[x0, y0]] grad_f = df(x0, y0) while ...\u0026gt;=eps: d = ... x0, y0 = ... evolution = ... grad_f = ... evolution_X = evolution[:, 0] evolution_Y = evolution[:, 1] x1 = np.linspace(-2, 2, 150) y1 = np.linspace(-2, 2, 150) Xx, Yy = np.meshgrid(x1, y1) Z = fxy(Xx, Yy) fig = plt.figure(figsize = (10,7)) contours = plt.contour(Xx, Yy, Z, 20) plt.clabel(contours, inline = True, fontsize = 10) plt.title(\u0026#34;Evolution of the cost function during gradient descent with level circles\u0026#34;, fontsize=15) plt.plot(evolution_X, evolution_Y) plt.plot(evolution_X, evolution_Y, \u0026#39;*\u0026#39;, label = \u0026#34;Cost function\u0026#34;) plt.xlabel(\u0026#39;x\u0026#39;, fontsize=11) plt.ylabel(\u0026#39;y\u0026#39;, fontsize=11) plt.colorbar() plt.legend(loc = \u0026#34;upper right\u0026#34;) plt.show() With $\\alpha=1$, the solution blows up, a correct choice for $\\alpha$ is very important. That is why, we will proceed with Armijo rule:\nc1=.6 beta=1. #test with beta=1 gamma=0.01 def phik(xyk,alphak,dk): return ... def dphik(xyk,alphak,dk): grad_f=... return ... def armijo_rule(alpha,xy_old,d): #d_x est la direction de descente d_x . grad_x \u0026lt;= 0 # test f(x_new) \\leq f(x_0) + c alpha ps{d_x}{grad_x} test = 1 iter=0 while test!=0 and iter\u0026lt;500: if phik(xy_old,alpha,d)\u0026lt;=...: test = 0 else: alpha*=gamma iter+=1 return alpha #initialization for Armijo alpha=beta ## start GD algorithm eps=1e-6 x0,y0=1,2 evolution = [[x0, y0]] grad_f = dfxy(x0, y0) cpt_grad=0 while norm(grad_f)\u0026gt;=eps and cpt_grad\u0026lt;500: d = -grad_f #armijo alpha=beta xy_old=np.array([x0,y0]) cpt=0 alpha=armijo_rule(alpha,xy_old,d) #while phik(xy_old,alpha,d)\u0026gt;phik(xy_old,0,d)+c1*dphik(xy_old,0,d)*alpha and cpt\u0026lt;500: # alpha*=gamma # cpt+=1 x0, y0 = x0 + alpha*d[0], y0 + alpha*d[1] evolution = np.vstack((evolution, [x0, y0])) grad_f = dfxy(x0, y0) cpt_grad+=1 evolution_X = evolution[:, 0] evolution_Y = evolution[:, 1] x1 = np.linspace(-2, 2, 150) y1 = np.linspace(-2, 2, 150) Xx, Yy = np.meshgrid(x1, y1) Z = fxy(Xx, Yy) fig = plt.figure(figsize = (10,7)) contours = …","date":1725566754,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725566754,"objectID":"5b4a949481c97ac3df94542bb48feedf","permalink":"https://example.com/post/notebook2/","publishdate":"2024-09-05T22:05:54+02:00","relpermalink":"/post/notebook2/","section":"post","summary":"Gradient descent To find the infimum of an arbitrary cost function, we use here the gradient descent. Let us consider the function $f(x,y)=2x^2+y^2-xy+2$ that we aim at minimizing on $ \\mathbb{R}^2$.","tags":[],"title":"Gradient descent","type":"post"},{"authors":[],"categories":[],"content":"#Install package import sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib !{sys.executable} -m pip install amplpy from amplpy import AMPL # import pyAMPL from amplpy import ampl_notebook ampl = ampl_notebook( modules=[\u0026#34;cplex\u0026#34;], # modules to install license_uuid=\u0026#34;default\u0026#34;, # license to use ) # instantiate AMPL object and register magics %%ampl_eval # define decision variables reset; # Declaration of optimization variables var xx; var yy; # Declaration of parameters param aa=-4; param bb=2; %%ampl_eval # Cost function minimize f: xx**2 + aa*(xx+yy) + 2*yy**2; # Constraints subject to g: xx+yy = bb; subject to h: xx \u0026gt;= 0; %%ampl_eval let xx:= 1; let yy:=2; # exhibit the model that has been built ampl.eval(\u0026#34;show;\u0026#34;) ampl.eval(\u0026#34;expand;\u0026#34;) # solve using two different solvers ampl.option[\u0026#34;solver\u0026#34;] = \u0026#34;cplex\u0026#34; ampl.solve() #ampl.option[\u0026#34;solver\u0026#34;] = \u0026#34;highs\u0026#34; #ampl.solve() ampl.display(\u0026#34;xx\u0026#34;);# xx,yy; ampl.display(\u0026#34;f\u0026#34;); ampl.display(\u0026#34;g.dual\u0026#34;); ampl.display(\u0026#34;h.dual\u0026#34;); ","date":1725566563,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725566563,"objectID":"3b6a8e6c6ea357c8a388cc5fa590c6ec","permalink":"https://example.com/post/notebook1/","publishdate":"2024-09-05T22:02:43+02:00","relpermalink":"/post/notebook1/","section":"post","summary":"#Install package import sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib !{sys.executable} -m pip install amplpy from amplpy import AMPL # import pyAMPL from amplpy import ampl_notebook ampl = ampl_notebook( modules=[\"cplex\"], # modules to install license_uuid=\"default\", # license to use ) # instantiate AMPL object and register magics %%ampl_eval # define decision variables reset; # Declaration of optimization variables var xx; var yy; # Declaration of parameters param aa=-4; param bb=2; %%ampl_eval # Cost function minimize f: xx**2 + aa*(xx+yy) + 2*yy**2; # Constraints subject to g: xx+yy = bb; subject to h: xx \u003e= 0; %%ampl_eval let xx:= 1; let yy:=2; # exhibit the model that has been built ampl.","tags":[],"title":"Notebook Pyampl tuto","type":"post"},{"authors":["Elise Grosjean","Alex Keilmann","Henry Jäger","Steffen Plunder","Claudia Redenbach","Bernd Simeon","Christina Surulescu"],"categories":[],"content":"","date":1717977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717977600,"objectID":"69c016b88deac2ee33b171f94a4c801b","permalink":"https://example.com/publication/publi7/","publishdate":"2024-06-10T00:00:00Z","relpermalink":"/publication/publi7/","section":"publication","summary":"We study the dynamics of a seeding experiment where a fibrous scaffold material is colonized by two types of cell populations. The specific application that we have in mind is related to the idea of meniscus tissue regeneration. In order to support the development of a promising replacement material, we discuss certain rate equations for the densities of human mesenchymal stem cells and chondrocytes and for the production of collagen-containing extracellular matrix. For qualitative studies, we start with a system of ordinary differential equations and refine then the model to include spatial effects of the underlying nonwoven scaffold structure. Numerical experiments as well as a complete set of parameters for future benchmarking are provided.","tags":[],"title":"Cell seeding dynamics in a porous scaffold material designed for meniscus tissue regeneration","type":"publication"},{"authors":["Elise Grosjean"],"categories":null,"content":"","date":1713744000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713744000,"objectID":"0b48aab82d484c3c9e3fb959be433ecf","permalink":"https://example.com/talk/seminaire-idefix-ensta-2024/","publishdate":"2024-04-22T00:00:00Z","relpermalink":"/talk/seminaire-idefix-ensta-2024/","section":"event","summary":"Présentation générale de mes travaux","tags":[],"title":"Séminaire Idefix ENSTA 2024","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":"","date":1710374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710374400,"objectID":"e983e000e8f114f09116cebdf69bf34b","permalink":"https://example.com/talk/seminaire-anedp-au-laboratoire-paul-painleve/","publishdate":"2024-03-14T00:00:00Z","relpermalink":"/talk/seminaire-anedp-au-laboratoire-paul-painleve/","section":"event","summary":"Analyse de sensibilité d'un modèle de régénération de tissu cellulaire et méthodes de base réduite non-intrusives deux-grilles","tags":[],"title":"Séminaire ANEDP au laboratoire Paul Painlevé","type":"event"},{"authors":["Elise Grosjean","Alex Keilmann","Henry Jäger","Shimi Mohanan","Claudia Redenbach","Bernd Simeon","Christina Surulescu","Luisa de Roy","Andreas Seitz","Graciosa Teixeira","Martin Dauner","Carsten Linti","Günter Schmidt"],"categories":[],"content":"","date":1709942400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709942400,"objectID":"d5cae5d870f64a3f441ae7d2513b1a12","permalink":"https://example.com/publication/publi6/","publishdate":"2024-03-09T00:00:00Z","relpermalink":"/publication/publi6/","section":"publication","summary":"We develop a model the dynamics of human mesenchymal stem cells (hMSCs) and chondrocytes evolving in a nonwoven polyethylene terephtalate (PET) scaffold impregnated with hyaluron and supplied with a differentiation medium. The scaffold and the cells are assumed to be contained in a bioreactor with fluid perfusion. The differentiation of hMSCs into chondrocytes favors the production of extracellular matrix (ECM) and is influenced by fluid stress. The model takes deformations of ECM and PET scaffold into account. The scaffold structure is explicitly included by statistical assessment of the fibre distribution from CT images. The effective macroscopic equations are obtained by appropriate upscaling from dynamics on lower (microscopic and mesoscopic) scales and feature in the motility terms an explicit cell diffusion tensor encoding the assessed anisotropic scaffold structure. Numerical simulations show its influence on the overall cell and tissue dynamics.","tags":[],"title":"An in-silico approach to meniscus tissue regeneration: Modeling, numerical simulation, and experimental analysis","type":"publication"},{"authors":["Elise Grosjean"],"categories":null,"content":"","date":1706486400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706486400,"objectID":"7d46a75cf09128e21c564baeffd77957","permalink":"https://example.com/talk/seminaire-lamfa/","publishdate":"2024-01-29T00:00:00Z","relpermalink":"/talk/seminaire-lamfa/","section":"event","summary":"Méthodes de base réduite non intrusives appliquée à l'analyse de sensibilité","tags":[],"title":"Séminaire Lamfa","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":"","date":1706140800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706140800,"objectID":"30806b3de7976043df768a898f6bebb1","permalink":"https://example.com/talk/seminaire-interne-m3disim-inria/","publishdate":"2024-01-25T00:00:00Z","relpermalink":"/talk/seminaire-interne-m3disim-inria/","section":"event","summary":"Sensitivity analysis with a RB approach","tags":[],"title":"Séminaire interne M3DISIM - INRIA","type":"event"},{"authors":["Elise Grosjean","Bernd Simeon"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"4e5417378d1c8634c4cc9f95768153c7","permalink":"https://example.com/publication/publi4/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/publi4/","section":"publication","summary":"This paper deals with the derivation of Non-Intrusive Reduced Basis (NIRB) techniques for sensitivity analysis, more specifically the direct and adjoint state methods. For highly complex parametric problems, these two approaches may become too costly. To reduce computational times, Proper Orthogonal Decomposition (POD) and Reduced Basis Methods (RBMs) have already been investigated. The majority of these algorithms are however intrusive in the sense that the High-Fidelity (HF) code must be modified. To address this issue, non-intrusive strategies are employed. The NIRB two-grid method uses the HF code solely as a ``black-box'', requiring no code modification. Like other RBMs, it is based on an offline-online decomposition. The offline stage is time-consuming, but it is only executed once, whereas the online stage is significantly less expensive than an HF evaluation. In this paper, we propose new NIRB two-grid algorithms for both the direct and adjoint state methods. On a classical model problem, the heat equation, we prove that HF evaluations of sensitivities reach an optimal convergence rate in L∞(0,T;H1(Ω)), and then establish that these rates are recovered by the proposed NIRB approximations. These results are supported by numerical simulations. We then numerically demonstrate that a further deterministic post-treatment can be applied to the direct method. This further reduces computational costs of the online step while only computing a coarse solution of the initial problem. All numerical results are run with the model problem as well as a more complex problem, namely the Brusselator system.","tags":[],"title":"Error estimate of the non-intrusive reduced basis two-grid method applied to sensitivity analysis (M2AN)","type":"publication"},{"authors":["Elise Grosjean","Yvon Maday"],"categories":[],"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701388800,"objectID":"1b5cc9e4fcdcba6d5677c2203abed7e4","permalink":"https://example.com/publication/publi3/","publishdate":"2023-12-01T00:00:00Z","relpermalink":"/publication/publi3/","section":"publication","summary":"Reduced Basis Methods (RBMs) are frequently proposed to approximate parametric problem solutions. They can be used to calculate solutions for a large number of parameter values (e.g. for parameter fitting) as well as to approximate a solution for a new parameter value (e.g. real time approximation with a very high accuracy). They intend to reduce the computational costs of High Fidelity (HF) codes. We will focus on the Non-Intrusive Reduced Basis (NIRB) two-grid method. Its main advantage is that it uses the HF code exclusively as a \"black-box,\" as opposed to other so-called intrusive methods that require code modification. This is very convenient when the HF code is a commercial one that has been purchased, as is frequently the case in the industry. The effectiveness of this method relies on its decomposition into two stages, one offline (classical in most RBMs as presented above) and one online. The offline part is time-consuming but it is only performed once. On the contrary, the specificity of this NIRB approach is that, during the online part, it solves the parametric problem on a coarse mesh only and then improves its precision. As a result, it is significantly less expensive than a HF evaluation. This method has been originally developed for elliptic equations with finite elements and has since been extended to finite volume. In this paper, we extend the NIRB two-grid method to parabolic equations. We recover optimal estimates in L∞(0,T;H1(Ω)) using as a model problem, the heat equation. Then, we present numerical results on the heat equation and on the Brusselator problem.","tags":[],"title":"Error estimate of the Non-Intrusive Reduced Basis (NIRB) two-grid method with parabolic equations, SMAI: JCM 9 (2023) 227-256","type":"publication"},{"authors":["Elise Grosjean"],"categories":null,"content":"","date":1695168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695168000,"objectID":"2dc838b15c2eced37e8a4a899f7dfb2f","permalink":"https://example.com/talk/iccb2023/","publishdate":"2023-09-20T00:00:00Z","relpermalink":"/talk/iccb2023/","section":"event","summary":"Meniscus tissue regeneration and sensitivity RB approach","tags":[],"title":"ICCB2023","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":" Presentation on the numerical aspects of a Coupled analysis of active biological processes for meniscus tissue regeneration Program ","date":1694476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694476800,"objectID":"0077ad4633c0f96be434e89651bfb232","permalink":"https://example.com/talk/spp2311-kick-off/","publishdate":"2023-09-12T00:00:00Z","relpermalink":"/talk/spp2311-kick-off/","section":"event","summary":"Coupled analysis of active biological processes for meniscus tissue regeneration","tags":[],"title":"SPP2311-Kick-off","type":"event"},{"authors":["Elise Grosjean","Bernd Simeon","Christina Surulescu"],"categories":[],"content":"","date":1692576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692576000,"objectID":"cf9d6b2158984e041997d7bb62d1e155","permalink":"https://example.com/publication/publi5/","publishdate":"2023-08-21T00:00:00Z","relpermalink":"/publication/publi5/","section":"publication","summary":"We propose a continuous model for meniscus cartilage regeneration triggered by two populations of cells migrating and (de)differentiating within an artificial scaffold with a known structure. The described biological processes are influenced by a fluid flow and therewith induced deformations of the scaffold. Numerical simulations are done for the corresponding dynamics within a bioreactor which was designed for performing the biological experiments.","tags":[],"title":"A mathematical model for meniscus cartilage regeneration (PAMM)","type":"publication"},{"authors":["Elise Grosjean"],"categories":null,"content":"","date":1685404800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685404800,"objectID":"9a09db21391e1d3a5fab65b2f69e3b53","permalink":"https://example.com/talk/gamm-annual-meeting-2023/","publishdate":"2023-05-30T00:00:00Z","relpermalink":"/talk/gamm-annual-meeting-2023/","section":"event","summary":"GAMM 2023","tags":[],"title":"GAMM Annual Meeting 2023","type":"event"},{"authors":["Elise Grosjean","Camille Pouchol"],"categories":null,"content":"","date":1672790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672790400,"objectID":"b0bd9fd8d6514f9ff7a8cb0a8b038c45","permalink":"https://example.com/talk/campus-france-presentation/","publishdate":"2023-01-04T00:00:00Z","relpermalink":"/talk/campus-france-presentation/","section":"event","summary":"Studying mathematics in France","tags":[],"title":"Campus France Presentation","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":" Abstract ","date":1669334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669334400,"objectID":"caf031fac111a02cf173c1ed70da7a66","permalink":"https://example.com/talk/map5-groupe-de-travail-modelisation-analyse-simulation/","publishdate":"2022-11-25T00:00:00Z","relpermalink":"/talk/map5-groupe-de-travail-modelisation-analyse-simulation/","section":"event","summary":"Meniscus tissue regeneration - sensibility analysis - non-intrusive reduced basis method","tags":[],"title":"MAP5 Groupe de travail Modelisation, Analyse, Simulation","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":"Differential-Algebraic Equations Website\n","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"7540a43924c6476503264dee5f496f37","permalink":"https://example.com/teaching/tuk/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/teaching/tuk/","section":"teaching","summary":"Differential-Algebraic Equations Website","tags":[],"title":"Tutor at Department of Mathematics, TU Kaiserslautern","type":"teaching"},{"authors":["Elise Grosjean"],"categories":null,"content":" Talk on non-intrusive reduced basis methods applied to parabolic equations. ","date":1655078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655078400,"objectID":"d3db4069812bf15464e91203772e23e8","permalink":"https://example.com/talk/canum-2022/","publishdate":"2022-06-13T00:00:00Z","relpermalink":"/talk/canum-2022/","section":"event","summary":" Talk on non-intrusive reduced basis methods applied to parabolic equations. ","tags":[],"title":"CANUM 2022","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":" Presentation on non intrusive reduced basis 2-grid method applied to wind farm simulations. Program ","date":1653868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653868800,"objectID":"e64723ea1a0bb8aa7dd4ca3eb08bc006","permalink":"https://example.com/talk/simulation-and-optimization-for-renewable-marine-energies/","publishdate":"2022-05-30T00:00:00Z","relpermalink":"/talk/simulation-and-optimization-for-renewable-marine-energies/","section":"event","summary":"Talk on Reduced basis method applied to wind farms","tags":[],"title":"Simulation and Optimization for Renewable Marine Energies","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":" Presentation on the numerical aspects of a Coupled analysis of active biological processes for meniscus tissue regeneration Program ","date":1653264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653264000,"objectID":"eb7a9a86204b8a46306b95b46b55d9bf","permalink":"https://example.com/talk/spp2311-kick-off/","publishdate":"2022-05-23T00:00:00Z","relpermalink":"/talk/spp2311-kick-off/","section":"event","summary":"Coupled analysis of active biological processes for meniscus tissue regeneration","tags":[],"title":"SPP2311-Kick-off","type":"event"},{"authors":["Elise Grosjean","Yvon Maday"],"categories":[],"content":"","date":1643673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643673600,"objectID":"9c373b2a3ef7cbfeb018c366ca19b9fd","permalink":"https://example.com/publication/publi2/","publishdate":"2022-02-01T00:00:00Z","relpermalink":"/publication/publi2/","section":"publication","summary":"Reduced Basis Methods (RBM) are often proposed to approximate the solutions of a parameter-dependent problem for a large number of parameter values, as an alternative to classical solvers, in order to reduce the computational costs. They usually are decomposed in two stages. One stage is done offline and can be considered as a learning procedure where a Reduced Basis (RB) is built from several solutions, called snapshots, computed with a high fidelity (HF) classical method, involving, e.g. a fine mesh (finite element or finite volume). In the second stage, which is online and has to be very cheap, a reduced basis problem is solved. The efficiency of the RBM relies on the ability, offline, to prepare the online step. In this article, we consider a Non-Intrusive RBM (NIRB) which is the two grids method. One fine mesh is used to construct the snapshots for the generation of the RB. Then, in the online part, the NIRB algorithm involves a coarse mesh where the problem solution for a new parameter is approximated and is $L^2$-projected on the RB. We adapt the NIRB algorithm to improve the accuracy and to further reduce the complexity of the online part, in particular by using a \\emph{domain truncation}. We exploit the fact that the solutions of parameterized problems behave physically similarly for a suitable range of parameters. We create two RB and a deterministic (algebraic) process which allows us to pass from one to the other. This new algorithm is applied to the 2D backward facing step. The second aim of this article is to deal with \\emph{singularities}. The domain geometry produces a fluid recirculation zone that must be captured correctly, for instance with a refinement of the mesh. The two-grid method in the FEM context is applied with a new stategy in order to counterbalance the effects of the singularities. The channel domain considered in the model problem has one re-entrant corner and thus the convergence is not optimal with uniform meshes. Thus, the theory of the two-grid method does not applied. However, we present several numerical results with fine refined meshes where both NIRB approaches succeed in retrieving the fine FEM accuracy.","tags":[],"title":"A doubly reduced approximation for the solution to PDEs based on a domain truncation and a reduced basis method: Application to Navier-Stokes equations (preprint)","type":"publication"},{"authors":["Elise Grosjean"],"categories":null,"content":" Presentation on optimizations of a non-intrusive reduced basis method Program ","date":1632182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632182400,"objectID":"5f725f33c23522440157b626409b9b9c","permalink":"https://example.com/talk/workshop-mathematics-of-high-performance-computing/","publishdate":"2021-09-21T00:00:00Z","relpermalink":"/talk/workshop-mathematics-of-high-performance-computing/","section":"event","summary":"Talk on optimizations of a non-intrusive reduced basis method","tags":[],"title":"Workshop Mathematics of High-Performance Computing","type":"event"},{"authors":["Elise Grosjean","Yvon Maday"],"categories":[],"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"a547b42cd6db23fe12bbe4f591488f4d","permalink":"https://example.com/publication/publi1/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/publication/publi1/","section":"publication","summary":"The context of this paper is the simulation of parameter-dependent partial differential equations (PDEs). When the aim is to solve such PDEs for a large number of parameter values, Reduced Basis Methods (RBM) are often used to reduce computational costs of a classical high fidelity code based on Finite Element Method (FEM), Finite Volume (FVM) or Spectral methods. The efficient implementation of most of these RBM requires to modify this high fidelity code, which cannot be done, for example in an industrial context if the high fidelity code is only accessible as a \"black-box\" solver. The Non-Intrusive Reduced Basis (NIRB) method has been introduced in the context of finite elements as a good alternative to reduce the implementation costs of these parameter-dependent problems. The method is efficient in other contexts than the FEM one, like with finite volume schemes, which are more often used in an industrial environment. In this case, some adaptations need to be done as the degrees of freedom in FV methods have different meanings. At this time, error estimates have only been studied with FEM solvers. In this paper, we present a generalisation of the NIRB method to Finite Volume schemes and we show that estimates established for FEM solvers also hold in the FVM setting. We first prove our results for the hybrid-Mimetic Finite Difference method (hMFD), which is part the Hybrid Mixed Mimetic methods (HMM) family. Then, we explain how these results apply more generally to other FV schemes. Some of them are specified, such as the Two Point Flux Approximation (TPFA).","tags":[],"title":"Error estimate of the Non-Intrusive Reduced Basis method with finite volume schemes, ESAIM: M2AN 55 (2021) 1941–1961","type":"publication"},{"authors":["Elise Grosjean"],"categories":null,"content":" Talk on non intrusive reduced basis methods applied to finite volume schemes. ","date":1606953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606953600,"objectID":"e8b1a738c1d699a9d5736e492fcceb27","permalink":"https://example.com/talk/can-j-2020/","publishdate":"2020-12-03T00:00:00Z","relpermalink":"/talk/can-j-2020/","section":"event","summary":" Talk on non intrusive reduced basis methods applied to finite volume schemes. ","tags":[],"title":"CAN-J 2020","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":" Presentation on non intrusive reduced basis methods applied to finite volume schemes. ","date":1602028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602028800,"objectID":"2789c7636ee8a9431a1e5184fa1c12e6","permalink":"https://example.com/talk/gtt-ljll/","publishdate":"2020-10-07T00:00:00Z","relpermalink":"/talk/gtt-ljll/","section":"event","summary":"Presentation on Reduced basis methods","tags":[],"title":"GTT LJLL","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":" Presentation on non intrusive reduced basis methods applied to finite volume schemes. Program ","date":1599609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599609600,"objectID":"3680febafcd08d785b512942f800e5d0","permalink":"https://example.com/talk/model-order-reduction-summer-school-morss2020/","publishdate":"2020-09-09T00:00:00Z","relpermalink":"/talk/model-order-reduction-summer-school-morss2020/","section":"event","summary":"Talk on Reduced basis method applied to wind farms","tags":[],"title":"Model Order Reduction Summer School MORSS2020","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":"link to ENSTA course description\nlink to tutorial : ampl and in python pyampl\nlink to first TP exercice: Gradient Descent TP\nlink to second TP exercice: Gradient Descent with Augmented Lagrangian\nlink to second TP exercice: Gradient Descent with Augmented Lagrangian (correction)\nlink to slides - part I : slides - part 1\nlink to slides - part II : slides - part 2\nlink to slides - part III : slides - part 3\nlink to TP - part I: TP 1 The goal of this first TP is to know how to launch jupyter notebook, save a notebook, launch amplpy package, model an optimization problem and solve it\nlink to TP - part I: TP 2\nUn corrigé d’exercice: Corrige\nSlides exercices en cours ici\nlink to exercice 6 : exo 6\nlink to exercice 7 : exo 7\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"486439fdd7e20059b52b3bb0dcd6b885","permalink":"https://example.com/teaching/ensta/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/teaching/ensta/","section":"teaching","summary":"link to ENSTA course description\nlink to tutorial : ampl and in python pyampl\nlink to first TP exercice: Gradient Descent TP\nlink to second TP exercice: Gradient Descent with Augmented Lagrangian","tags":[],"title":"APM_5EN5A_TA at ENSTA-Paris","type":"teaching"},{"authors":["Elise Grosjean"],"categories":null,"content":"link to ENSAE website\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"14207e61e9f0b1c43a0b4fc657a59504","permalink":"https://example.com/teaching/ensae/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/teaching/ensae/","section":"teaching","summary":"link to ENSAE website","tags":[],"title":"Tutor at l'école nationale de la statistique et de l'administration économique (ENSAE)","type":"teaching"},{"authors":["Elise Grosjean"],"categories":null,"content":" Poster presentation on non intrusive reduced basis methods applied to wind farms. Painting exhibition (see above) ","date":1574812800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574812800,"objectID":"97adff086625845d3bcc9d91c8cb7079","permalink":"https://example.com/talk/poster-session-50-ljll/","publishdate":"2019-11-27T00:00:00Z","relpermalink":"/talk/poster-session-50-ljll/","section":"event","summary":"Poster presentation on Reduced basis method applied to wind farms","tags":[],"title":"Poster Session 50 LJLL","type":"event"},{"authors":["Elise Grosjean"],"categories":null,"content":" 4M026 Approximation des équations aux derivées partielles\nAnalyse numérique matricielle, TD/TP L3.\nAnalyse numérique pour les EDO, TP L3.\nProgrammation Python\n","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"549ae6a66febaa1770b1caf5cc874db2","permalink":"https://example.com/teaching/sorbonne/","publishdate":"2018-09-01T00:00:00Z","relpermalink":"/teaching/sorbonne/","section":"teaching","summary":"TDs et TPs (monitorat)","tags":[],"title":"Tutor at Sorbonne université","type":"teaching"},{"authors":null,"categories":null,"content":"Gradient descent with projection and augmented lagrangian algorithm To find the infimum of an arbitrary cost function, we use here the gradient descent. Let us consider the function $f(x,y)=0.5(x^2+(y-1)^2)$ that we aim at minimizing on $K\\subset \\mathbb{R}^2$ with $K={(x,y) | x= y}$. We start by manually compute its gradient and we will display its evolution during gradient descent.\nimport sys !{sys.executable} -m pip install numpy !{sys.executable} -m pip install matplotlib 1. Complete the following code import numpy as np import matplotlib.pyplot as plt def f(x1,x2): # function return ... def df(x1,x2): # gradient return ... def g(x1,x2): # constraint return ... def dg(x1,x2): # constraint gradient return ... def L(x1,x2,mu): # lagrangian return ... def Lc(x1,x2,mu,c): # augmented lagrangian return ... def dxLc(x1,x2,mu,c): # augmented lagrangian gradient return ... We need to define a norm to now how far from the global solution we are.\ndef norm(a): return np.sqrt(a[0]**2+a[1]**2) With $\\alpha$ too big, we found out that the solution blows up in the previous exercice, a correct choice for $\\alpha$ is very important. That is why, we will proceed with Armijo-Wolfe rule:\n# Wolfe rule: c1=.6 c2=.8 beta=1. #test with beta=1 eta=2. gamma=0.01 def phik(xyk,alphak,dk,fc,mu,c): return fc(xyk[0]+alphak*dk[0],xyk[1]+alphak*dk[1],mu,c) def dphik(xyk,alphak,dk,dfc,mu,c): grad_f=dfc(xyk[0]+alphak*dk[0],xyk[1]+alphak*dk[1],mu,c) return np.dot(grad_f,dk) def wolfe_rule(alpha,xy_old,d,fc,dfc,mu=0,c=0): #d_x est la direction de descente d_x . grad_x \u0026lt;= 0 # test f(x_new) \\leq f(x_0) + c alpha ps{d_x}{grad_x} test = 1 iteration = 0 min_ = 0 max_ = 1000 while (phik(xy_old,alpha,d,fc,mu,c)\u0026lt;=(phik(xy_old,0,d,fc,mu,c)+c1*dphik(xy_old,0,d,dfc,mu,c)*alpha)) \u0026amp;(iteration\u0026lt;=500): #armijo ok alpha=eta*alpha iteration = 0 while (test!=0)\u0026amp;(iteration\u0026lt;=500): xnew0,xnew1=xy_old[0]+alpha*d[0],xy_old[1]+alpha*d[1] if (phik(xy_old,alpha,d,fc,mu,c)\u0026lt;= (phik(xy_old,0,d,fc,mu,c)+c1*dphik(xy_old,0,d,dfc,mu,c)*alpha)) \u0026amp; (np.dot(dfc(xnew0,xnew1,mu,c),d) \u0026gt;= c2*np.dot(dfc(xy_old[0],xy_old[1],mu,c),d) ): test = 0 elif phik(xy_old,alpha,d,fc,mu,c)\u0026gt; (phik(xy_old,0,d,fc,mu,c)+c1*dphik(xy_old,0,d,dfc,mu,c)*alpha): #no armijo max_ = alpha alpha = (max_ + min_)/2 iteration = iteration +1 else: # armijo ok minorant = alpha alpha = (max_ + min_)/2 iteration = iteration +1 return alpha 2. Now we proceed with the Gradient descent algorithm with Armijo-Wolfe rule: Complete the following code def GradientDescent(fc,dfc,xy0,beta,mu=0,c=0): #initialization for Armijo alpha=beta # start GD algorithm eps=1e-6 x0,y0=xy0[0],xy0[1] #starting points evolution = [[x0, y0]] grad_f = ... #gradient function of Lc cpt_grad=0 while norm(grad_f)\u0026gt;=eps and cpt_grad\u0026lt;500: d = ... #direction alpha=beta #armijo xy_old=np.array([x0,y0]) alpha=wolfe_rule(alpha,xy_old,d,fc,dfc, mu=mu,c=c) # find best alpha x0, y0 = ... evolution = np.vstack((evolution, [x0, y0])) grad_f = ... cpt_grad+=1 return evolution 3. Complete the code of the augmented lagrangian: #initialization for Armijo alpha=beta ## start GD algorithm eps=1e-6 # initial points x1,x2=1.,1. c=1. # penalty parameter mu=1.7 # dual variable gold=... # not a golden function, but the constraints are very precious gradx_f = ... # gradient of the augmented lagrangian cpt=0 # to avoid infinite while loop xy0=[x1,x2] while norm(gradx_f)\u0026gt;eps or np.abs(g(x1,x2))\u0026gt;eps : evolution=... evolution_X = evolution[:, 0] evolution_Y = evolution[:, 1] x1,x2=evolution_X[-1],evolution_Y[-1] # last points print(\u0026#34;x1:\u0026#34;, x1, \u0026#34; x2:\u0026#34;, x2) if (np.abs(g(x1,x2)))\u0026lt;np.abs(gold): # if ||g|| is small #print(\u0026#34;||g|| smaller\u0026#34;,np.abs(g(x1,x2))) mu=... #increase mu print(\u0026#34;new mu: \u0026#34;, mu) else: c=... # increase c print(\u0026#34;new c: \u0026#34;,c) cpt+=1 gold=... #updates gradx_f = ... print(\u0026#34;norm of gradx_f: \u0026#34;,norm(gradx_f)) if cpt==20: break We can display the solution onto the contour of $f$ or onto the contour of $L_c$\nevolution_X = evolution[:, 0] evolution_Y = evolution[:, 1] x1 = np.linspace(-2, 2, 25) y1 = np.linspace(-2, 2, 25) Xx, Yy = np.meshgrid(x1, y1) print(\u0026#34;mu\u0026#34;,mu) #mu=1.#0.5 #Z = Lc(Xx, Yy,mu=mu,c=0) Z = f(Xx, Yy) fig= plt.figure(figsize = (10,7)) contours = plt.contour(Xx, Yy, Z, 45) plt.clabel(contours, inline = True, fontsize = 10) plt.title(\u0026#34;Evolution of the cost function during gradient descent with level circles\u0026#34;, fontsize=15) plt.plot(evolution_X, evolution_Y) plt.plot(evolution_X, evolution_Y, \u0026#39;*\u0026#39;, label = \u0026#34;Cost function\u0026#34;) plt.xlabel(\u0026#39;x\u0026#39;, fontsize=11) plt.ylabel(\u0026#39;y\u0026#39;, fontsize=11) plt.colorbar() plt.legend(loc = \u0026#34;upper right\u0026#34;) plt.show() mu 0.5000001501871618 4. We add new constraints: $(x,y)$ must stay in $K=[-2.,0]\\times [-2,0]$. Complete the code that finds the projection of $(x,y)$ on a cuboid $K$. Then, change the gradient descent algorithm to take into account this projection step def proj(K,x): #K=[[l0,u0],[l1,u1]] l0=... u0=... l1=... u1=... return ...,... # NEW GRADIENT DESCENT ALGORITHM: def GradientDescent(fc,dfc,xy0,beta,mu=0,c=0): #initialization for …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9110f9c5ebe81c74f6a55f3326d453aa","permalink":"https://example.com/post/notebook3/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/notebook3/","section":"post","summary":"Gradient descent with projection and augmented lagrangian algorithm To find the infimum of an arbitrary cost function, we use here the gradient descent. Let us consider the function $f(x,y)=0.5(x^2+(y-1)^2)$ that we aim at minimizing on $K\\subset \\mathbb{R}^2$ with $K={(x,y) | x= y}$.","tags":null,"title":"","type":"post"}]